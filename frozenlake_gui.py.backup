"""
FrozenLake Interactive GUI Application

A user-friendly interface for students to experiment with Q-learning
and observe how different hyperparameters affect agent performance.
"""

import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
import gymnasium as gym
import numpy as np
import threading
import time
from queue import Queue
import random
import sys


class ToolTip:
    """Create a tooltip for a given widget."""
    def __init__(self, widget, text):
        self.widget = widget
        self.text = text
        self.tooltip = None
        self.widget.bind("<Enter>", self.show_tooltip)
        self.widget.bind("<Leave>", self.hide_tooltip)

    def show_tooltip(self, event=None):
        x, y, _, _ = self.widget.bbox("insert")
        x += self.widget.winfo_rootx() + 25
        y += self.widget.winfo_rooty() + 25

        self.tooltip = tk.Toplevel(self.widget)
        self.tooltip.wm_overrideredirect(True)
        self.tooltip.wm_geometry(f"+{x}+{y}")

        label = tk.Label(self.tooltip, text=self.text, justify='left',
                        background="#ffffe0", relief='solid', borderwidth=1,
                        font=("Arial", 9), wraplength=300)
        label.pack()

    def hide_tooltip(self, event=None):
        if self.tooltip:
            self.tooltip.destroy()
            self.tooltip = None


def has_valid_path(grid):
    """Check if there's a valid path from start (0,0) to goal using BFS."""
    size = len(grid)
    start = (0, 0)
    goal = (size - 1, size - 1)

    # BFS to find path
    from collections import deque
    queue = deque([start])
    visited = {start}

    # Directions: left, down, right, up
    directions = [(0, -1), (1, 0), (0, 1), (-1, 0)]

    while queue:
        row, col = queue.popleft()

        # Check if we reached the goal
        if (row, col) == goal:
            return True

        # Explore neighbors
        for dr, dc in directions:
            new_row, new_col = row + dr, col + dc

            # Check bounds
            if 0 <= new_row < size and 0 <= new_col < size:
                if (new_row, new_col) not in visited:
                    # Check if it's not a hole
                    if grid[new_row][new_col] != 'H':
                        visited.add((new_row, new_col))
                        queue.append((new_row, new_col))

    return False


def generate_random_map(size=8, hole_probability=0.2, max_attempts=100):
    """Generate a random valid FrozenLake map with guaranteed path to goal."""
    if size < 4:
        size = 4

    attempts = 0
    while attempts < max_attempts:
        attempts += 1

        # Start with all frozen
        grid = [['F' for _ in range(size)] for _ in range(size)]

        # Set start and goal
        grid[0][0] = 'S'
        grid[size-1][size-1] = 'G'

        # Add holes randomly
        for i in range(size):
            for j in range(size):
                if grid[i][j] == 'F' and random.random() < hole_probability:
                    grid[i][j] = 'H'

        # Check if there's a valid path
        if has_valid_path(grid):
            return [''.join(row) for row in grid]

    # Fallback: create a simple guaranteed path if max attempts reached
    grid = [['F' for _ in range(size)] for _ in range(size)]
    grid[0][0] = 'S'
    grid[size-1][size-1] = 'G'

    # Create a safe path along the edge
    for i in range(size):
        grid[i][0] = 'F' if grid[i][0] == 'F' else grid[i][0]
        grid[size-1][i] = 'F' if grid[size-1][i] == 'F' else grid[size-1][i]

    # Add some random holes away from the safe path
    for i in range(1, size - 1):
        for j in range(2, size - 1):
            if random.random() < hole_probability * 0.5:
                grid[i][j] = 'H'

    return [''.join(row) for row in grid]


class QLearningAgent:
    """Q-Learning agent implementation with customizable action biases and advanced penalties."""

    def __init__(self, env, learning_rate=0.1, discount_factor=0.99,
                 epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01,
                 action_biases=None, reward_shaping=None, advanced_penalties=None):
        self.env = env
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.q_table = np.zeros([env.observation_space.n, env.action_space.n])

        # Action biases: [left, down, right, up] - higher = more likely
        self.action_biases = action_biases if action_biases is not None else [1.0, 1.0, 1.0, 1.0]

        # Reward shaping: custom rewards
        self.reward_shaping = reward_shaping if reward_shaping is not None else {
            'goal': 1.0,
            'hole': 0.0,
            'step': 0.0  # Penalty per step
        }

        # Advanced penalties: custom penalties for specific behaviors
        self.advanced_penalties = advanced_penalties if advanced_penalties is not None else {
            'wall_hit': 0.0,       # Penalty for hitting wall (staying in same position)
            'revisit': 0.0,        # Penalty for revisiting a position
            'loop': 0.0,           # Penalty for looping (visiting same position twice in short time)
            'distance': 0.0        # Bonus/penalty based on distance to goal
        }

        # Tracking for episode
        self.visited_states = set()
        self.recent_states = []  # For loop detection
        self.last_state = None

    def choose_action(self, state, greedy=False):
        """Choose action using epsilon-greedy policy with action biases."""
        if greedy or np.random.random() >= self.epsilon:
            # Exploitation: choose best action with bias
            q_values = self.q_table[state].copy()
            # Apply biases to Q-values
            biased_q = q_values * self.action_biases
            return np.argmax(biased_q)
        else:
            # Exploration: random with bias
            biases = np.array(self.action_biases)
            probs = biases / biases.sum()
            return np.random.choice(4, p=probs)

    def update_q_table(self, state, action, reward, next_state, done):
        """Update Q-table using Q-learning formula with reward shaping and advanced penalties."""
        # Apply reward shaping
        shaped_reward = reward
        if reward > 0:  # Goal reached
            shaped_reward = self.reward_shaping['goal']
        elif done and reward == 0:  # Hole
            shaped_reward = self.reward_shaping['hole']
        shaped_reward += self.reward_shaping['step']  # Step penalty/bonus

        # Apply advanced penalties
        # Wall hit detection (stayed in same position)
        if self.last_state is not None and state == next_state and not done:
            shaped_reward += self.advanced_penalties['wall_hit']

        # Revisit penalty
        if next_state in self.visited_states:
            shaped_reward += self.advanced_penalties['revisit']

        # Loop detection (visited same state in last 5 steps)
        if len(self.recent_states) >= 5 and next_state in self.recent_states[-5:]:
            shaped_reward += self.advanced_penalties['loop']

        # Distance-based reward (Manhattan distance to goal)
        if self.advanced_penalties['distance'] != 0.0:
            grid_size = int(np.sqrt(self.env.observation_space.n))
            goal_state = self.env.observation_space.n - 1

            next_row, next_col = divmod(next_state, grid_size)
            goal_row, goal_col = divmod(goal_state, grid_size)
            distance = abs(next_row - goal_row) + abs(next_col - goal_col)

            # Negative distance means closer to goal = positive reward
            shaped_reward += self.advanced_penalties['distance'] * (-distance / (grid_size * 2))

        # Update tracking
        self.visited_states.add(next_state)
        self.recent_states.append(next_state)
        if len(self.recent_states) > 10:
            self.recent_states.pop(0)
        self.last_state = next_state

        current_q = self.q_table[state, action]
        if done:
            target_q = shaped_reward
        else:
            target_q = shaped_reward + self.gamma * np.max(self.q_table[next_state])
        self.q_table[state, action] = current_q + self.lr * (target_q - current_q)

    def reset_episode_tracking(self):
        """Reset tracking variables for new episode."""
        self.visited_states = set()
        self.recent_states = []
        self.last_state = None

    def decay_epsilon(self):
        """Decay exploration rate."""
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)


class FrozenLakeGUI:
    """Main GUI application for FrozenLake training and visualization."""

    def __init__(self, root):
        self.root = root
        self.root.title("FrozenLake Q-Learning Lab - Interface PÃ©dagogique")
        self.root.geometry("1500x900")

        # Training state
        self.agent = None
        self.env = None
        self.is_training = False
        self.training_thread = None
        self.message_queue = Queue()
        self.custom_map = None
        self.rewards_history = []
        self.last_graph_update = 0  # Track last graph update to avoid frequent updates

        # UI Mode
        self.advanced_mode = False  # Start in simple mode

        # Setup GUI
        self.setup_gui()
        self.check_queue()

    def setup_gui(self):
        """Setup all GUI components."""

        # Main container
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        # Configure grid weights
        self.root.columnconfigure(0, weight=1)
        self.root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.columnconfigure(1, weight=2)
        main_frame.rowconfigure(1, weight=1)

        # Title with help button
        title_frame = ttk.Frame(main_frame)
        title_frame.grid(row=0, column=0, columnspan=2, pady=10)

        title_label = ttk.Label(title_frame, text="ğŸ§Š FrozenLake Q-Learning Lab ğŸ¤–",
                                font=('Arial', 20, 'bold'))
        title_label.pack(side=tk.LEFT, padx=10)

        help_button = ttk.Button(title_frame, text="â“ Guide d'Utilisation",
                                command=self.show_help)
        help_button.pack(side=tk.LEFT)

        # Left panel - Parameters
        self.setup_parameters_panel(main_frame)

        # Right panel - Visualization and controls
        self.setup_visualization_panel(main_frame)

        # Bottom panel - Console output
        self.setup_console_panel(main_frame)

    def show_help(self):
        """Show help window with explanations."""
        help_window = tk.Toplevel(self.root)
        help_window.title("Guide d'Utilisation - Q-Learning")
        help_window.geometry("800x600")
        help_window.transient(self.root)

        text = scrolledtext.ScrolledText(help_window, wrap=tk.WORD, font=('Arial', 10))
        text.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)

        help_text = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           GUIDE D'UTILISATION - FROZENLAKE Q-LEARNING LAB             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“– QU'EST-CE QUE LE Q-LEARNING?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Le Q-Learning est une technique d'apprentissage par renforcement oÃ¹ un agent
apprend Ã  naviguer dans un environnement en essayant diffÃ©rentes actions et
en apprenant de leurs rÃ©sultats (rÃ©compenses).

ğŸ® LE JEU FROZENLAKE
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
- S (Start) : Point de dÃ©part
- F (Frozen): Glace sÃ»re, on peut marcher dessus
- H (Hole)  : Trou dans la glace, GAME OVER!
- G (Goal)  : Objectif Ã  atteindre = +1 point

L'agent doit apprendre Ã  aller de S Ã  G sans tomber dans les trous (H).

âš™ï¸ COMPRENDRE LES HYPERPARAMÃˆTRES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“Š TAUX D'APPRENTISSAGE (Î± - Alpha)
   Valeur: 0.01 Ã  1.0 | RecommandÃ©: 0.1-0.3

   â€¢ Plus il est Ã‰LEVÃ‰ (0.5-1.0):
     â†’ Apprend vite mais peut Ãªtre instable
     â†’ Bonne pour des tests rapides
     â†’ Peut "oublier" ce qu'il a appris avant

   â€¢ Plus il est BAS (0.01-0.2):
     â†’ Apprend lentement mais de faÃ§on stable
     â†’ Bon pour l'apprentissage final
     â†’ MÃ©morise mieux les expÃ©riences

   ğŸ¯ CONSEIL: Commencez avec 0.2 pour voir des rÃ©sultats rapidement!

ğŸ’° FACTEUR DE DISCOUNT (Î³ - Gamma)
   Valeur: 0.0 Ã  1.0 | RecommandÃ©: 0.95-0.99

   â€¢ Ã‰LEVÃ‰ (0.95-0.99):
     â†’ L'agent planifie Ã  long terme
     â†’ Pense aux rÃ©compenses futures
     â†’ Meilleur pour trouver le chemin optimal

   â€¢ BAS (0.5-0.8):
     â†’ L'agent est "myope"
     â†’ Ne regarde que les rÃ©compenses immÃ©diates
     â†’ Peut rater le meilleur chemin

   ğŸ¯ CONSEIL: Utilisez 0.95 ou plus pour FrozenLake!

ğŸ² DÃ‰CROISSANCE EPSILON (Exploration vs Exploitation)
   Valeur: 0.90 Ã  0.999 | RecommandÃ©: 0.995-0.999

   L'epsilon contrÃ´le l'exploration:
   â€¢ DÃ©but: epsilon = 1.0 â†’ 100% d'exploration (actions alÃ©atoires)
   â€¢ Fin: epsilon â†’ 0.01 â†’ 99% exploitation (utilise ce qu'il a appris)

   â€¢ DÃ‰CROISSANCE RAPIDE (0.95-0.98):
     â†’ ArrÃªte d'explorer rapidement
     â†’ Converge vite mais peut rater de meilleures solutions

   â€¢ DÃ‰CROISSANCE LENTE (0.995-0.999):
     â†’ Continue d'explorer longtemps
     â†’ Trouve souvent de meilleurs chemins
     â†’ NÃ©cessite plus d'Ã©pisodes

   ğŸ¯ CONSEIL: Utilisez 0.995 pour un bon Ã©quilibre!

ğŸ“ˆ NOMBRE D'Ã‰PISODES
   RecommandÃ©: 5000-15000 pour 4x4, 20000+ pour 8x8

   Un Ã©pisode = une partie complÃ¨te (de S Ã  G ou Ã  un trou)

   â€¢ TROP PEU (<2000):
     â†’ L'agent n'a pas assez appris
     â†’ Taux de rÃ©ussite faible

   â€¢ OPTIMAL (5000-15000):
     â†’ Bon Ã©quilibre temps/performance
     â†’ L'agent a le temps d'apprendre

   ğŸ¯ CONSEIL: 10000 Ã©pisodes est un bon point de dÃ©part!

ğŸŒŠ GLACE GLISSANTE

   â€¢ ACTIVÃ‰E (Stochastique):
     â†’ L'agent ne va pas toujours oÃ¹ il veut (33% de chance)
     â†’ Plus rÃ©aliste et difficile
     â†’ NÃ©cessite plus d'entraÃ®nement

   â€¢ DÃ‰SACTIVÃ‰E (DÃ©terministe):
     â†’ L'agent va exactement oÃ¹ il veut
     â†’ Plus facile Ã  apprendre
     â†’ Bon pour dÃ©buter et comprendre

   ğŸ¯ CONSEIL: Commencez SANS glace glissante pour voir l'apprentissage!

ğŸ“Š INTERPRÃ‰TATION DES RÃ‰SULTATS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… BON APPRENTISSAGE:
   â€¢ Taux de rÃ©ussite: >70% (sans glace), >60% (avec glace)
   â€¢ Epsilon final: <0.1 (a fini d'explorer)
   â€¢ Courbe: monte progressivement

âŒ APPRENTISSAGE INSUFFISANT:
   â€¢ Taux de rÃ©ussite: <30%
   â€¢ Possible raisons:
     â†’ Pas assez d'Ã©pisodes
     â†’ Taux d'apprentissage trop bas
     â†’ DÃ©croissance epsilon trop rapide

ğŸ¯ PRÃ‰RÃ‰GLAGES RECOMMANDÃ‰S
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ“ DÃ‰BUTANT (Pour comprendre):
   â€¢ Sans glace glissante
   â€¢ 5000 Ã©pisodes
   â€¢ Î±=0.2, Î³=0.95
   â€¢ RÃ©sultat attendu: >90%

âš¡ STANDARD (Ã‰quilibrÃ©):
   â€¢ Avec glace glissante
   â€¢ 10000 Ã©pisodes
   â€¢ Î±=0.15, Î³=0.98
   â€¢ RÃ©sultat attendu: 65-75%

ğŸ¯ OPTIMAL (Meilleure performance):
   â€¢ Avec glace glissante
   â€¢ 15000 Ã©pisodes
   â€¢ Î±=0.1, Î³=0.99
   â€¢ RÃ©sultat attendu: 75-85%

ğŸ’¡ CONSEILS PRATIQUES
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1. Commencez avec le prÃ©rÃ©glage DÃ‰BUTANT pour voir que Ã§a marche!
2. Activez ensuite la glace glissante pour le vrai challenge
3. Augmentez les Ã©pisodes si le taux de rÃ©ussite est trop bas
4. Regardez la courbe d'apprentissage dans le graphique
5. La dÃ©mo visuelle montre comment l'agent a appris Ã  jouer

ğŸ› PROBLÃˆMES COURANTS
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â“ L'agent n'apprend pas (taux <10%)?
   â†’ Augmentez le nombre d'Ã©pisodes Ã  15000+
   â†’ Augmentez le taux d'apprentissage Ã  0.2-0.3
   â†’ Ralentissez la dÃ©croissance epsilon (0.997+)

â“ L'apprentissage est instable?
   â†’ RÃ©duisez le taux d'apprentissage (0.1)
   â†’ Augmentez le facteur de discount (0.99)

â“ Ã‡a prend trop de temps?
   â†’ RÃ©duisez le nombre d'Ã©pisodes
   â†’ Augmentez la frÃ©quence de mise Ã  jour UI
   â†’ Utilisez le prÃ©rÃ©glage RAPIDE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
        text.insert('1.0', help_text)
        text.config(state='disabled')

    def setup_parameters_panel(self, parent):
        """Setup hyperparameter controls panel with scrollbar."""

        # Container frame
        container = ttk.LabelFrame(parent, text="âš™ï¸ ParamÃ¨tres d'Apprentissage", padding="5")
        container.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=5)
        container.rowconfigure(0, weight=1)
        container.columnconfigure(0, weight=1)

        # Canvas with scrollbar
        canvas = tk.Canvas(container, highlightthickness=0, width=380)
        scrollbar = ttk.Scrollbar(container, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)

        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )

        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)

        canvas.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))

        # Enable mouse wheel scrolling
        def on_mousewheel(event):
            canvas.yview_scroll(int(-1*(event.delta/120)), "units")
        canvas.bind_all("<MouseWheel>", on_mousewheel)

        # Now use scrollable_frame as the parent for all parameters
        params_frame = scrollable_frame
        row = 0

        # Environment settings
        env_label = ttk.Label(params_frame, text="ğŸ® Environnement:",
                             font=('Arial', 10, 'bold'))
        env_label.grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 10))
        ToolTip(env_label, "Configuration de la carte de jeu")
        row += 1

        # Map size
        map_label = ttk.Label(params_frame, text="Taille de la carte:")
        map_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(map_label, "4x4 = facile et rapide\n8x8 = plus difficile\nPersonnalisÃ©e = crÃ©ez votre propre carte")

        self.map_size = tk.StringVar(value="4x4")
        map_combo = ttk.Combobox(params_frame, textvariable=self.map_size,
                                values=["4x4", "8x8", "PersonnalisÃ©e"], state="readonly", width=15)
        map_combo.grid(row=row, column=1, sticky=tk.W, pady=5)
        map_combo.bind('<<ComboboxSelected>>', self.on_map_size_change)
        row += 1

        # Random map button
        self.random_map_button = ttk.Button(params_frame, text="ğŸ² GÃ©nÃ©rer Carte AlÃ©atoire",
                                           command=self.generate_random_map, state=tk.DISABLED)
        self.random_map_button.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        row += 1

        # Slippery
        slippery_frame = ttk.Frame(params_frame)
        slippery_frame.grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=5)
        self.is_slippery = tk.BooleanVar(value=False)  # Changed default to False
        slippery_check = ttk.Checkbutton(slippery_frame, text="Glace glissante",
                                        variable=self.is_slippery)
        slippery_check.pack(side=tk.LEFT)
        ToolTip(slippery_check, "â„ï¸ DÃ‰SACTIVÃ‰ (recommandÃ© pour dÃ©buter):\n"
                "L'agent va exactement oÃ¹ vous voulez\n\n"
                "â„ï¸ ACTIVÃ‰ (plus difficile):\n"
                "33% de chance d'aller dans une direction alÃ©atoire")
        row += 1

        # Separator
        ttk.Separator(params_frame, orient='horizontal').grid(row=row, column=0, columnspan=2,
                                                              sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Hyperparameters
        hyper_label = ttk.Label(params_frame, text="ğŸ§  HyperparamÃ¨tres:",
                               font=('Arial', 10, 'bold'))
        hyper_label.grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 10))
        row += 1

        # Learning rate
        lr_label = ttk.Label(params_frame, text="Taux d'apprentissage (Î±):")
        lr_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(lr_label, "Vitesse d'apprentissage\n\n"
                "Ã‰LEVÃ‰ (0.3-0.5): Apprend vite mais instable\n"
                "BAS (0.1-0.2): Apprend lentement mais stable\n\n"
                "ğŸ’¡ RecommandÃ©: 0.15-0.2")

        self.learning_rate = tk.DoubleVar(value=0.15)
        lr_scale = ttk.Scale(params_frame, from_=0.01, to=0.5, variable=self.learning_rate,
                            orient=tk.HORIZONTAL, length=150)
        lr_scale.grid(row=row, column=1, sticky=tk.W, pady=5)
        row += 1
        self.lr_label = ttk.Label(params_frame, text="0.15", foreground='blue')
        self.lr_label.grid(row=row, column=1, sticky=tk.W)
        self.learning_rate.trace_add('write', lambda *args: self.lr_label.config(
            text=f"{self.learning_rate.get():.2f}"))
        row += 1

        # Discount factor
        gamma_label = ttk.Label(params_frame, text="Facteur de discount (Î³):")
        gamma_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(gamma_label, "Importance du futur\n\n"
                "Ã‰LEVÃ‰ (0.95-0.99): Planifie Ã  long terme\n"
                "BAS (0.5-0.8): Myope, rÃ©compense immÃ©diate\n\n"
                "ğŸ’¡ RecommandÃ©: 0.95-0.99")

        self.discount_factor = tk.DoubleVar(value=0.98)
        gamma_scale = ttk.Scale(params_frame, from_=0.5, to=0.99, variable=self.discount_factor,
                               orient=tk.HORIZONTAL, length=150)
        gamma_scale.grid(row=row, column=1, sticky=tk.W, pady=5)
        row += 1
        self.gamma_label = ttk.Label(params_frame, text="0.98", foreground='blue')
        self.gamma_label.grid(row=row, column=1, sticky=tk.W)
        self.discount_factor.trace_add('write', lambda *args: self.gamma_label.config(
            text=f"{self.discount_factor.get():.2f}"))
        row += 1

        # Epsilon decay
        decay_label = ttk.Label(params_frame, text="DÃ©croissance epsilon:")
        decay_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(decay_label, "Vitesse de passage explorationâ†’exploitation\n\n"
                "RAPIDE (0.95-0.98): ArrÃªte d'explorer vite\n"
                "LENT (0.995-0.999): Continue d'explorer\n\n"
                "ğŸ’¡ RecommandÃ©: 0.995-0.997")

        self.epsilon_decay = tk.DoubleVar(value=0.996)
        decay_scale = ttk.Scale(params_frame, from_=0.95, to=0.999, variable=self.epsilon_decay,
                               orient=tk.HORIZONTAL, length=150)
        decay_scale.grid(row=row, column=1, sticky=tk.W, pady=5)
        row += 1
        self.decay_label = ttk.Label(params_frame, text="0.996", foreground='blue')
        self.decay_label.grid(row=row, column=1, sticky=tk.W)
        self.epsilon_decay.trace_add('write', lambda *args: self.decay_label.config(
            text=f"{self.epsilon_decay.get():.3f}"))
        row += 1

        # Episodes
        ep_label = ttk.Label(params_frame, text="Nombre d'Ã©pisodes:")
        ep_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(ep_label, "Nombre de parties d'entraÃ®nement\n\n"
                "COURT (<5000): Rapide mais peu efficace\n"
                "MOYEN (10000): Bon Ã©quilibre\n"
                "LONG (>15000): Meilleure performance\n\n"
                "ğŸ’¡ RecommandÃ©: 10000")

        self.episodes = tk.IntVar(value=10000)
        episodes_entry = ttk.Entry(params_frame, textvariable=self.episodes, width=17)
        episodes_entry.grid(row=row, column=1, sticky=tk.W, pady=5)
        row += 1

        # Update frequency
        update_label = ttk.Label(params_frame, text="Mise Ã  jour UI:")
        update_label.grid(row=row, column=0, sticky=tk.W, pady=5)
        ToolTip(update_label, "FrÃ©quence de rafraÃ®chissement de l'interface\n\n"
                "BAS (10-50): Plus fluide, peut ralentir\n"
                "MOYEN (100): Bon Ã©quilibre\n"
                "HAUT (500+): Moins fluide, plus rapide")

        self.update_frequency = tk.IntVar(value=100)
        update_combo = ttk.Combobox(params_frame, textvariable=self.update_frequency,
                                   values=[50, 100, 250, 500], state="readonly", width=15)
        update_combo.grid(row=row, column=1, sticky=tk.W, pady=5)
        row += 1

        # Separator
        ttk.Separator(params_frame, orient='horizontal').grid(row=row, column=0, columnspan=2,
                                                              sticky=(tk.W, tk.E), pady=10)
        row += 1

        # Preset buttons
        preset_label = ttk.Label(params_frame, text="ğŸ¯ PrÃ©rÃ©glages:", font=('Arial', 10, 'bold'))
        preset_label.grid(row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 5))
        row += 1

        beginner_btn = ttk.Button(params_frame, text="ğŸ“ DÃ©butant (Facile, rapide)",
                  command=self.preset_beginner)
        beginner_btn.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=2)
        ToolTip(beginner_btn, "Sans glace glissante\n5000 Ã©pisodes\nRÃ©sultat attendu: >90%")
        row += 1

        standard_btn = ttk.Button(params_frame, text="âš¡ Standard (Ã‰quilibrÃ©)",
                  command=self.preset_standard)
        standard_btn.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=2)
        ToolTip(standard_btn, "Avec glace glissante\n10000 Ã©pisodes\nRÃ©sultat attendu: 65-75%")
        row += 1

        optimal_btn = ttk.Button(params_frame, text="ğŸ¯ Optimal (Meilleure perf)",
                  command=self.preset_optimal)
        optimal_btn.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=2)
        ToolTip(optimal_btn, "Avec glace glissante\n15000 Ã©pisodes\nRÃ©sultat attendu: 75-85%")
        row += 1

        # Advanced ludic controls
        ttk.Separator(params_frame, orient=tk.HORIZONTAL).grid(row=row, column=0, columnspan=2,
                                                                sticky=(tk.W, tk.E), pady=10)
        row += 1

        ttk.Label(params_frame, text="ğŸ® Mode Ludique", font=('Arial', 11, 'bold')).grid(
            row=row, column=0, columnspan=2, sticky=tk.W, pady=(0, 5))
        row += 1

        # Action weights panel
        action_frame = ttk.LabelFrame(params_frame, text="âš¡ Poids des Actions", padding="15")
        action_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        action_frame.columnconfigure(1, weight=1)
        row += 1

        # Create a visual directional pad
        self.action_weights = {
            'left': tk.DoubleVar(value=1.0),
            'down': tk.DoubleVar(value=1.0),
            'right': tk.DoubleVar(value=1.0),
            'up': tk.DoubleVar(value=1.0)
        }

        # Visual grid layout for directional pad
        ttk.Label(action_frame, text="â† Gauche:", font=('Arial', 9)).grid(row=0, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        left_spin = ttk.Spinbox(action_frame, from_=0.1, to=5.0, increment=0.1,
                                textvariable=self.action_weights['left'], width=10)
        left_spin.grid(row=0, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(left_spin, "Poids pour aller Ã  gauche\n>1.0 = favorisÃ©\n<1.0 = dÃ©favorisÃ©")

        ttk.Label(action_frame, text="â†“ Bas:", font=('Arial', 9)).grid(row=1, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        down_spin = ttk.Spinbox(action_frame, from_=0.1, to=5.0, increment=0.1,
                                textvariable=self.action_weights['down'], width=10)
        down_spin.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(down_spin, "Poids pour aller vers le bas\n>1.0 = favorisÃ©\n<1.0 = dÃ©favorisÃ©")

        ttk.Label(action_frame, text="â†’ Droite:", font=('Arial', 9)).grid(row=2, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        right_spin = ttk.Spinbox(action_frame, from_=0.1, to=5.0, increment=0.1,
                                 textvariable=self.action_weights['right'], width=10)
        right_spin.grid(row=2, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(right_spin, "Poids pour aller Ã  droite\n>1.0 = favorisÃ©\n<1.0 = dÃ©favorisÃ©")

        ttk.Label(action_frame, text="â†‘ Haut:", font=('Arial', 9)).grid(row=3, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        up_spin = ttk.Spinbox(action_frame, from_=0.1, to=5.0, increment=0.1,
                              textvariable=self.action_weights['up'], width=10)
        up_spin.grid(row=3, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(up_spin, "Poids pour aller vers le haut\n>1.0 = favorisÃ©\n<1.0 = dÃ©favorisÃ©")

        # Visual probability display
        self.action_prob_label = ttk.Label(action_frame, text="ProbabilitÃ©s: â† 25% | â†“ 25% | â†’ 25% | â†‘ 25%",
                                           font=('Arial', 8), foreground='blue')
        self.action_prob_label.grid(row=4, column=0, columnspan=2, pady=(8, 0))
        ToolTip(self.action_prob_label, "ProbabilitÃ©s relatives pendant l'exploration alÃ©atoire")

        # Update probability display when weights change
        for weight_var in self.action_weights.values():
            weight_var.trace_add('write', self.update_action_probabilities)

        # Reward shaping panel
        reward_frame = ttk.LabelFrame(params_frame, text="ğŸ RÃ©compenses PersonnalisÃ©es", padding="15")
        reward_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        reward_frame.columnconfigure(1, weight=1)
        row += 1

        self.rewards = {
            'goal': tk.DoubleVar(value=1.0),
            'hole': tk.DoubleVar(value=0.0),
            'step': tk.DoubleVar(value=0.0)
        }

        ttk.Label(reward_frame, text="ğŸ† Objectif atteint:", font=('Arial', 9)).grid(
            row=0, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        goal_spin = ttk.Spinbox(reward_frame, from_=-5.0, to=10.0, increment=0.1,
                                textvariable=self.rewards['goal'], width=10)
        goal_spin.grid(row=0, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(goal_spin, "RÃ©compense pour atteindre le but\nDÃ©faut: 1.0\nPlus haut = plus motivÃ©")

        ttk.Label(reward_frame, text="ğŸ’€ Tombe dans trou:", font=('Arial', 9)).grid(
            row=1, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        hole_spin = ttk.Spinbox(reward_frame, from_=-5.0, to=5.0, increment=0.1,
                                textvariable=self.rewards['hole'], width=10)
        hole_spin.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(hole_spin, "PÃ©nalitÃ© pour tomber dans un trou\nDÃ©faut: 0.0\nNÃ©gatif = pÃ©nalise plus")

        ttk.Label(reward_frame, text="ğŸ‘£ Chaque pas:", font=('Arial', 9)).grid(
            row=2, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        step_spin = ttk.Spinbox(reward_frame, from_=-1.0, to=1.0, increment=0.01,
                                textvariable=self.rewards['step'], width=10)
        step_spin.grid(row=2, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(step_spin, "RÃ©compense/pÃ©nalitÃ© par pas\nDÃ©faut: 0.0\nNÃ©gatif = encourage rapiditÃ©")

        # Advanced penalties panel
        penalties_frame = ttk.LabelFrame(params_frame, text="âš™ï¸ PÃ©nalitÃ©s AvancÃ©es", padding="15")
        penalties_frame.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)
        penalties_frame.columnconfigure(1, weight=1)
        row += 1

        self.advanced_penalties = {
            'wall_hit': tk.DoubleVar(value=0.0),
            'revisit': tk.DoubleVar(value=0.0),
            'loop': tk.DoubleVar(value=0.0),
            'distance': tk.DoubleVar(value=0.0)
        }

        ttk.Label(penalties_frame, text="ğŸ§± Collision mur:", font=('Arial', 9)).grid(
            row=0, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        wall_spin = ttk.Spinbox(penalties_frame, from_=-1.0, to=1.0, increment=0.01,
                                textvariable=self.advanced_penalties['wall_hit'], width=10)
        wall_spin.grid(row=0, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(wall_spin, "PÃ©nalitÃ© quand l'agent reste sur place (mur)\nDÃ©faut: 0.0\nNÃ©gatif = pÃ©nalise les collisions")

        ttk.Label(penalties_frame, text="ğŸ” Revisite case:", font=('Arial', 9)).grid(
            row=1, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        revisit_spin = ttk.Spinbox(penalties_frame, from_=-1.0, to=1.0, increment=0.01,
                                   textvariable=self.advanced_penalties['revisit'], width=10)
        revisit_spin.grid(row=1, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(revisit_spin, "PÃ©nalitÃ© pour visiter une case dÃ©jÃ  visitÃ©e\nDÃ©faut: 0.0\nNÃ©gatif = encourage exploration")

        ttk.Label(penalties_frame, text="ğŸ”„ Boucle dÃ©tectÃ©e:", font=('Arial', 9)).grid(
            row=2, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        loop_spin = ttk.Spinbox(penalties_frame, from_=-1.0, to=1.0, increment=0.01,
                                textvariable=self.advanced_penalties['loop'], width=10)
        loop_spin.grid(row=2, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(loop_spin, "PÃ©nalitÃ© pour boucler (mÃªme case dans les 5 derniers pas)\nDÃ©faut: 0.0\nNÃ©gatif = pÃ©nalise les boucles")

        ttk.Label(penalties_frame, text="ğŸ“ Distance au but:", font=('Arial', 9)).grid(
            row=3, column=0, sticky=tk.W, pady=3, padx=(0, 10))
        distance_spin = ttk.Spinbox(penalties_frame, from_=-2.0, to=2.0, increment=0.1,
                                    textvariable=self.advanced_penalties['distance'], width=10)
        distance_spin.grid(row=3, column=1, sticky=(tk.W, tk.E), pady=3)
        ToolTip(distance_spin, "Bonus basÃ© sur la distance au but\nDÃ©faut: 0.0\nPositif = rÃ©compense rapprochement\nNÃ©gatif = pÃ©nalise Ã©loignement")

        # Reset button
        reset_btn = ttk.Button(params_frame, text="ğŸ”„ RÃ©initialiser Mode Ludique",
                               command=self.reset_ludic_mode)
        reset_btn.grid(row=row, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(10, 20), padx=5)
        ToolTip(reset_btn, "Remet tous les poids et rÃ©compenses Ã  1.0")

    def update_action_probabilities(self, *args):
        """Update the visual display of action probabilities."""
        weights = [self.action_weights['left'].get(),
                   self.action_weights['down'].get(),
                   self.action_weights['right'].get(),
                   self.action_weights['up'].get()]
        total = sum(weights)
        if total > 0:
            probs = [w / total * 100 for w in weights]
            self.action_prob_label.config(
                text=f"ProbabilitÃ©s: â† {probs[0]:.0f}% | â†“ {probs[1]:.0f}% | â†’ {probs[2]:.0f}% | â†‘ {probs[3]:.0f}%"
            )

    def reset_ludic_mode(self):
        """Reset all ludic mode parameters to default."""
        for weight_var in self.action_weights.values():
            weight_var.set(1.0)
        self.rewards['goal'].set(1.0)
        self.rewards['hole'].set(0.0)
        self.rewards['step'].set(0.0)
        self.advanced_penalties['wall_hit'].set(0.0)
        self.advanced_penalties['revisit'].set(0.0)
        self.advanced_penalties['loop'].set(0.0)
        self.advanced_penalties['distance'].set(0.0)
        self.log_message("âœ… Mode ludique rÃ©initialisÃ© aux valeurs par dÃ©faut")

    def show_learning_curve(self):
        """Display learning curve in the embedded graph panel."""
        # Don't update if there's no data or too little data
        if not self.rewards_history or len(self.rewards_history) < 10:
            return

        try:
            import matplotlib
            matplotlib.use('TkAgg')  # Ensure TkAgg backend is used
            import matplotlib.pyplot as plt
            from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
            plt.ioff()  # Turn off interactive mode to avoid threading issues

            # Clear existing canvas if any
            if self.graph_canvas:
                self.graph_canvas.get_tk_widget().destroy()

            # Calculate moving average
            window_size = 100
            if len(self.rewards_history) < window_size:
                window_size = max(10, len(self.rewards_history) // 10)

            moving_avg = []
            for i in range(len(self.rewards_history)):
                if i < window_size:
                    moving_avg.append(np.mean(self.rewards_history[:i+1]) * 100)
                else:
                    moving_avg.append(np.mean(self.rewards_history[i-window_size+1:i+1]) * 100)

            # Create figure (smaller for embedded view)
            self.graph_figure = plt.Figure(figsize=(6, 8), dpi=80)

            # Plot 1: Win rate over time
            ax1 = self.graph_figure.add_subplot(2, 1, 1)
            episodes = list(range(1, len(moving_avg) + 1))
            ax1.plot(episodes, moving_avg, color='#2ecc71', linewidth=2)
            ax1.fill_between(episodes, 0, moving_avg, alpha=0.3, color='#2ecc71')
            ax1.set_xlabel('Ã‰pisode', fontsize=10)
            ax1.set_ylabel('Taux (%)', fontsize=10)
            ax1.set_title(f'Taux de RÃ©ussite (moy. {window_size})', fontsize=11, fontweight='bold')
            ax1.grid(True, alpha=0.3)
            ax1.set_ylim(0, 100)
            ax1.tick_params(labelsize=8)

            # Plot 2: Success/Failure per episode (raw data sampled)
            ax2 = self.graph_figure.add_subplot(2, 1, 2)
            sample_rate = max(1, len(self.rewards_history) // 500)
            sampled_episodes = episodes[::sample_rate]
            sampled_rewards = [r * 100 for r in self.rewards_history[::sample_rate]]

            colors = ['#27ae60' if r > 0 else '#e74c3c' for r in sampled_rewards]
            ax2.bar(sampled_episodes, sampled_rewards, color=colors, alpha=0.6, width=sample_rate*0.8)
            ax2.set_xlabel('Ã‰pisode', fontsize=10)
            ax2.set_ylabel('RÃ©sultat', fontsize=10)
            ax2.set_title('RÃ©sultats par Ã‰pisode', fontsize=11, fontweight='bold')
            ax2.set_yticks([0, 100])
            ax2.set_yticklabels(['Ã‰chec', 'SuccÃ¨s'], fontsize=8)
            ax2.grid(True, alpha=0.3, axis='x')
            ax2.tick_params(labelsize=8)

            self.graph_figure.tight_layout(pad=2.0)

            # Embed in tkinter frame
            self.graph_canvas = FigureCanvasTkAgg(self.graph_figure, master=self.graph_frame_widget)
            self.graph_canvas.draw()
            self.graph_canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

            self.log("ğŸ“Š Graphique d'apprentissage mis Ã  jour!")

        except ImportError:
            self.log("âš ï¸  Matplotlib n'est pas installÃ©. Installez-le avec: pip install matplotlib")
        except Exception as e:
            self.log(f"âŒ Erreur lors de la crÃ©ation du graphique: {str(e)}")
            import traceback
            self.log(traceback.format_exc())

    def setup_visualization_panel(self, parent):
        """Setup visualization and control panel."""

        viz_frame = ttk.Frame(parent)
        viz_frame.grid(row=1, column=1, sticky=(tk.W, tk.E, tk.N, tk.S), padx=5, pady=5)
        viz_frame.rowconfigure(1, weight=1)
        viz_frame.rowconfigure(2, weight=1)
        viz_frame.columnconfigure(0, weight=1)
        viz_frame.columnconfigure(1, weight=1)  # Add second column for graph

        # Control buttons (span 2 columns)
        control_frame = ttk.LabelFrame(viz_frame, text="ğŸ® ContrÃ´les", padding="10")
        control_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))

        self.train_button = ttk.Button(control_frame, text="ğŸš€ DÃ©marrer l'EntraÃ®nement",
                                       command=self.start_training)
        self.train_button.grid(row=0, column=0, padx=5, pady=5)

        self.stop_button = ttk.Button(control_frame, text="â¹ï¸ ArrÃªter",
                                      command=self.stop_training, state=tk.DISABLED)
        self.stop_button.grid(row=0, column=1, padx=5, pady=5)

        self.demo_button = ttk.Button(control_frame, text="ğŸ‘ï¸ DÃ©mo dans la Grille",
                                      command=self.show_grid_demo, state=tk.DISABLED)
        self.demo_button.grid(row=0, column=2, padx=5, pady=5)
        ToolTip(self.demo_button, "Animation de l'agent dans la grille ci-dessous")

        self.pygame_button = ttk.Button(control_frame, text="ğŸ® DÃ©mo Pygame",
                                        command=self.show_pygame_demo, state=tk.DISABLED)
        self.pygame_button.grid(row=0, column=3, padx=5, pady=5)
        ToolTip(self.pygame_button, "Ouvre une fenÃªtre de jeu pygame")

        # Demo speed control
        speed_label = ttk.Label(control_frame, text="Vitesse:")
        speed_label.grid(row=0, column=4, padx=(20, 5))
        ToolTip(speed_label, "Vitesse de la dÃ©mo\n0.1 = trÃ¨s rapide\n1.0 = lent")

        self.demo_speed = tk.DoubleVar(value=0.3)
        speed_combo = ttk.Combobox(control_frame, textvariable=self.demo_speed,
                                  values=[0.1, 0.2, 0.3, 0.5, 1.0], state="readonly", width=8)
        speed_combo.grid(row=0, column=5, padx=5)

        # Live visualization toggle
        self.show_live_grid = tk.BooleanVar(value=True)
        live_check = ttk.Checkbutton(control_frame, text="Voir entraÃ®nement",
                                     variable=self.show_live_grid)
        live_check.grid(row=0, column=6, padx=(20, 5))
        ToolTip(live_check, "Affiche l'agent dans la grille pendant l'entraÃ®nement")

        # Statistics panel
        stats_frame = ttk.LabelFrame(viz_frame, text="ğŸ“Š Statistiques en Temps RÃ©el", padding="10")
        stats_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        stats_frame.columnconfigure(0, weight=1)

        # Stats display
        stats_container = ttk.Frame(stats_frame)
        stats_container.pack(fill=tk.BOTH, expand=True)

        # Progress
        ttk.Label(stats_container, text="Progression:", font=('Arial', 10, 'bold')).grid(
            row=0, column=0, sticky=tk.W, pady=5)
        self.progress_var = tk.StringVar(value="0 / 0 Ã©pisodes")
        ttk.Label(stats_container, textvariable=self.progress_var).grid(
            row=0, column=1, sticky=tk.W, padx=10, pady=5)

        self.progress_bar = ttk.Progressbar(stats_container, mode='determinate', length=400)
        self.progress_bar.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5, padx=5)

        # Epsilon
        eps_label = ttk.Label(stats_container, text="Epsilon (exploration):", font=('Arial', 10, 'bold'))
        eps_label.grid(row=2, column=0, sticky=tk.W, pady=5)
        ToolTip(eps_label, "Taux d'exploration actuel\n1.0 = 100% exploration\n0.01 = 99% exploitation")

        self.epsilon_var = tk.StringVar(value="1.000")
        ttk.Label(stats_container, textvariable=self.epsilon_var, font=('Arial', 12)).grid(
            row=2, column=1, sticky=tk.W, padx=10, pady=5)

        # Win rate
        win_label = ttk.Label(stats_container, text="Taux de rÃ©ussite:", font=('Arial', 10, 'bold'))
        win_label.grid(row=3, column=0, sticky=tk.W, pady=5)
        ToolTip(win_label, "% de victoires sur les 100 derniers Ã©pisodes\n>70% = trÃ¨s bon (sans glace)\n>60% = trÃ¨s bon (avec glace)")

        self.winrate_var = tk.StringVar(value="0.0%")
        self.winrate_label = ttk.Label(stats_container, textvariable=self.winrate_var,
                 font=('Arial', 14, 'bold'), foreground='green')
        self.winrate_label.grid(row=3, column=1, sticky=tk.W, padx=10, pady=5)

        # Average reward
        ttk.Label(stats_container, text="RÃ©compense moyenne:",
                 font=('Arial', 10, 'bold')).grid(row=4, column=0, sticky=tk.W, pady=5)
        self.reward_var = tk.StringVar(value="0.000")
        ttk.Label(stats_container, textvariable=self.reward_var, font=('Arial', 12)).grid(
            row=4, column=1, sticky=tk.W, padx=10, pady=5)

        # Time elapsed
        ttk.Label(stats_container, text="Temps Ã©coulÃ©:", font=('Arial', 10, 'bold')).grid(
            row=5, column=0, sticky=tk.W, pady=5)
        self.time_var = tk.StringVar(value="0s")
        ttk.Label(stats_container, textvariable=self.time_var).grid(
            row=5, column=1, sticky=tk.W, padx=10, pady=5)

        # ETA
        ttk.Label(stats_container, text="Temps restant:", font=('Arial', 10, 'bold')).grid(
            row=6, column=0, sticky=tk.W, pady=5)
        self.eta_var = tk.StringVar(value="--")
        ttk.Label(stats_container, textvariable=self.eta_var).grid(
            row=6, column=1, sticky=tk.W, padx=10, pady=5)

        # Grid visualization (left side, smaller)
        grid_frame = ttk.LabelFrame(viz_frame, text="ğŸ® Visualisation de la Grille", padding="10")
        grid_frame.grid(row=2, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(0, 5))

        self.grid_canvas = tk.Canvas(grid_frame, width=400, height=400, bg='#2c3e50')
        self.grid_canvas.pack(fill=tk.BOTH, expand=True)

        # Graph visualization (right side)
        graph_frame = ttk.LabelFrame(viz_frame, text="ğŸ“ˆ Courbe d'Apprentissage", padding="10")
        graph_frame.grid(row=2, column=1, sticky=(tk.W, tk.E, tk.N, tk.S), padx=(5, 0))

        # Create matplotlib canvas
        self.graph_canvas = None
        self.graph_figure = None
        self.graph_frame_widget = graph_frame  # Store reference

        # Legend
        legend_frame = ttk.Frame(grid_frame)
        legend_frame.pack(fill=tk.X, pady=5)

        ttk.Label(legend_frame, text="ğŸŸ¦ Start", foreground='blue').pack(side=tk.LEFT, padx=5)
        ttk.Label(legend_frame, text="ğŸŸ© Goal", foreground='green').pack(side=tk.LEFT, padx=5)
        ttk.Label(legend_frame, text="â¬œ Safe", foreground='gray').pack(side=tk.LEFT, padx=5)
        ttk.Label(legend_frame, text="â¬› Hole", foreground='black').pack(side=tk.LEFT, padx=5)
        ttk.Label(legend_frame, text="ğŸ”´ Agent", foreground='red').pack(side=tk.LEFT, padx=5)

    def setup_console_panel(self, parent):
        """Setup console output panel."""

        console_frame = ttk.LabelFrame(parent, text="ğŸ“ Journal d'EntraÃ®nement", padding="10")
        console_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S),
                          padx=5, pady=5)
        console_frame.rowconfigure(0, weight=1)
        console_frame.columnconfigure(0, weight=1)

        self.console = scrolledtext.ScrolledText(console_frame, height=8, wrap=tk.WORD,
                                                 font=('Courier', 9))
        self.console.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))

        self.log("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        self.log("â•‘  Bienvenue dans le FrozenLake Q-Learning Lab! ğŸ§ŠğŸ¤–                 â•‘")
        self.log("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        self.log("")
        self.log("ğŸ’¡ DÃ‰MARRAGE RAPIDE:")
        self.log("   1. Cliquez sur 'ğŸ“ DÃ©butant' pour charger des paramÃ¨tres qui marchent")
        self.log("   2. Cliquez sur 'ğŸš€ DÃ©marrer l'EntraÃ®nement'")
        self.log("   3. Regardez le taux de rÃ©ussite monter!")
        self.log("   4. Cliquez sur 'ğŸ‘ï¸ Voir DÃ©mo Visuelle' pour voir l'agent jouer")
        self.log("")
        self.log("â“ Cliquez sur 'â“ Guide d'Utilisation' pour comprendre les paramÃ¨tres!")
        self.log("-" * 72)

    def log(self, message):
        """Add message to console."""
        self.console.insert(tk.END, message + "\n")
        self.console.see(tk.END)

    def draw_grid(self, agent_pos=None, path=None, show_values=False):
        """Draw the FrozenLake grid with agent position."""
        if not self.env:
            # Draw placeholder
            self.grid_canvas.delete("all")
            width = self.grid_canvas.winfo_width()
            height = self.grid_canvas.winfo_height()
            if width > 10 and height > 10:
                self.grid_canvas.create_text(width/2, height/2,
                    text="La grille apparaÃ®tra aprÃ¨s l'entraÃ®nement",
                    fill='white', font=('Arial', 12))
            return

        self.grid_canvas.delete("all")
        width = self.grid_canvas.winfo_width()
        height = self.grid_canvas.winfo_height()

        if width < 50 or height < 50:
            return

        # Get grid description
        if self.custom_map:
            desc = self.custom_map
        else:
            map_name = self.map_size.get()
            if map_name == "8x8":
                desc = ["SFFFFFFF", "FFFFFFFF", "FFFHFFFF", "FFFFFHFF",
                        "FFFHFFFF", "FHHFFFHF", "FHFFHFHF", "FFFHFFFG"]
            else:  # 4x4
                desc = ["SFFF", "FHFH", "FFFH", "HFFG"]

        grid_size = len(desc)
        margin = 20
        available_width = width - 2 * margin
        available_height = height - 2 * margin
        cell_size = min(available_width / grid_size, available_height / grid_size)

        # Center the grid
        start_x = margin + (available_width - cell_size * grid_size) / 2
        start_y = margin + (available_height - cell_size * grid_size) / 2

        # Draw cells
        for row in range(grid_size):
            for col in range(grid_size):
                x1 = start_x + col * cell_size
                y1 = start_y + row * cell_size
                x2 = x1 + cell_size
                y2 = y1 + cell_size

                cell = desc[row][col]

                # Cell colors
                if cell == 'S':
                    color = '#3498db'  # Blue
                    text_color = 'white'
                    label = 'S'
                elif cell == 'G':
                    color = '#2ecc71'  # Green
                    text_color = 'white'
                    label = 'G'
                elif cell == 'H':
                    color = '#34495e'  # Dark gray
                    text_color = 'white'
                    label = 'H'
                else:  # F
                    color = '#ecf0f1'  # Light gray
                    text_color = 'black'
                    label = ''

                # Draw cell
                self.grid_canvas.create_rectangle(x1, y1, x2, y2,
                    fill=color, outline='#2c3e50', width=2)

                # Draw label
                if label:
                    self.grid_canvas.create_text(x1 + cell_size/2, y1 + cell_size/2,
                        text=label, font=('Arial', int(cell_size/3), 'bold'),
                        fill=text_color)

                # Show Q-values if requested
                if show_values and self.agent and cell == 'F':
                    state = row * grid_size + col
                    max_q = np.max(self.agent.q_table[state])
                    if max_q > 0:
                        # Draw best action arrow
                        best_action = np.argmax(self.agent.q_table[state])
                        cx = x1 + cell_size/2
                        cy = y1 + cell_size/2
                        arrow_len = cell_size/4

                        arrows = {
                            0: (-arrow_len, 0),  # Left
                            1: (0, arrow_len),   # Down
                            2: (arrow_len, 0),   # Right
                            3: (0, -arrow_len)   # Up
                        }
                        dx, dy = arrows[best_action]
                        self.grid_canvas.create_line(cx, cy, cx+dx, cy+dy,
                            arrow=tk.LAST, fill='#e74c3c', width=2)

        # Draw agent
        if agent_pos is not None:
            row = agent_pos // grid_size
            col = agent_pos % grid_size
            x = start_x + col * cell_size + cell_size/2
            y = start_y + row * cell_size + cell_size/2
            radius = cell_size/3

            # Agent as a red circle
            self.grid_canvas.create_oval(x-radius, y-radius, x+radius, y+radius,
                fill='#e74c3c', outline='#c0392b', width=3)

        # Draw path if provided
        if path:
            for i in range(len(path)-1):
                row1 = path[i] // grid_size
                col1 = path[i] % grid_size
                row2 = path[i+1] // grid_size
                col2 = path[i+1] % grid_size

                x1 = start_x + col1 * cell_size + cell_size/2
                y1 = start_y + row1 * cell_size + cell_size/2
                x2 = start_x + col2 * cell_size + cell_size/2
                y2 = start_y + row2 * cell_size + cell_size/2

                self.grid_canvas.create_line(x1, y1, x2, y2,
                    fill='#f39c12', width=3, arrow=tk.LAST)

    def on_map_size_change(self, event=None):
        """Handle map size change."""
        if self.map_size.get() == "PersonnalisÃ©e":
            self.random_map_button.config(state=tk.NORMAL)
        else:
            self.random_map_button.config(state=tk.DISABLED)
            self.custom_map = None

    def generate_random_map(self):
        """Generate a random map."""
        try:
            dialog = tk.Toplevel(self.root)
            dialog.title("GÃ©nÃ©rer Carte AlÃ©atoire")
            dialog.geometry("350x220")
            dialog.transient(self.root)
            dialog.grab_set()

            ttk.Label(dialog, text="Taille de la carte:", font=('Arial', 10, 'bold')).pack(pady=10)
            size_var = tk.IntVar(value=8)
            size_scale = ttk.Scale(dialog, from_=4, to=12, variable=size_var, orient=tk.HORIZONTAL, length=250)
            size_scale.pack(pady=5)
            size_label = ttk.Label(dialog, text="8x8")
            size_label.pack()
            size_var.trace_add('write', lambda *args: size_label.config(text=f"{size_var.get()}x{size_var.get()}"))

            ttk.Label(dialog, text="ProbabilitÃ© de trous:", font=('Arial', 10, 'bold')).pack(pady=10)
            prob_var = tk.DoubleVar(value=0.2)
            prob_scale = ttk.Scale(dialog, from_=0.1, to=0.4, variable=prob_var, orient=tk.HORIZONTAL, length=250)
            prob_scale.pack(pady=5)
            prob_label = ttk.Label(dialog, text="0.20")
            prob_label.pack()
            prob_var.trace_add('write', lambda *args: prob_label.config(text=f"{prob_var.get():.2f}"))

            def on_generate():
                size = size_var.get()
                prob = prob_var.get()
                self.log(f"\nğŸ² GÃ©nÃ©ration d'une carte {size}x{size} (probabilitÃ© trous: {prob:.2f})...")

                # Track attempts for logging
                attempts = 0
                max_attempts = 100

                while attempts < max_attempts:
                    attempts += 1
                    candidate_map = generate_random_map(size, prob, max_attempts=1)

                    # Verify path exists
                    grid = [list(row) for row in candidate_map]
                    if has_valid_path(grid):
                        self.custom_map = candidate_map
                        self.log(f"âœ… Carte valide gÃ©nÃ©rÃ©e en {attempts} tentative{'s' if attempts > 1 else ''}")
                        self.log(f"   Chemin garanti de S (dÃ©part) Ã  G (arrivÃ©e)")
                        for i, row in enumerate(self.custom_map):
                            self.log(f"  {row}")
                        dialog.destroy()
                        return

                # Fallback if max attempts reached (shouldn't happen often)
                self.log(f"âš ï¸  DifficultÃ© Ã  gÃ©nÃ©rer avec ces paramÃ¨tres")
                self.log(f"   GÃ©nÃ©ration d'une carte garantie avec chemin...")
                self.custom_map = generate_random_map(size, prob, max_attempts=100)
                self.log(f"âœ… Carte de secours gÃ©nÃ©rÃ©e (chemin garanti)")
                for i, row in enumerate(self.custom_map):
                    self.log(f"  {row}")
                dialog.destroy()

            ttk.Button(dialog, text="GÃ©nÃ©rer", command=on_generate).pack(pady=15)

        except Exception as e:
            self.log(f"âŒ Erreur gÃ©nÃ©ration carte: {e}")

    def preset_beginner(self):
        """Load beginner preset - GUARANTEED TO WORK!"""
        self.learning_rate.set(0.2)
        self.discount_factor.set(0.95)
        self.epsilon_decay.set(0.997)
        self.episodes.set(5000)
        self.update_frequency.set(50)
        self.map_size.set("4x4")
        self.is_slippery.set(False)  # NO SLIPPERY!
        self.on_map_size_change()
        self.log("\nğŸ“ PRÃ‰RÃ‰GLAGE DÃ‰BUTANT chargÃ©")
        self.log("   â€¢ Sans glace glissante (plus facile!)")
        self.log("   â€¢ 5000 Ã©pisodes (rapide)")
        self.log("   â€¢ RÃ©sultat attendu: >90% de rÃ©ussite")
        self.log("   â¡ï¸  Cliquez sur 'ğŸš€ DÃ©marrer l'EntraÃ®nement'")

    def preset_standard(self):
        """Load standard preset."""
        self.learning_rate.set(0.15)
        self.discount_factor.set(0.98)
        self.epsilon_decay.set(0.996)
        self.episodes.set(10000)
        self.update_frequency.set(100)
        self.map_size.set("4x4")
        self.is_slippery.set(True)  # WITH SLIPPERY
        self.on_map_size_change()
        self.log("\nâš¡ PRÃ‰RÃ‰GLAGE STANDARD chargÃ©")
        self.log("   â€¢ Avec glace glissante (plus difficile)")
        self.log("   â€¢ 10000 Ã©pisodes")
        self.log("   â€¢ RÃ©sultat attendu: 65-75% de rÃ©ussite")

    def preset_optimal(self):
        """Load optimal preset."""
        self.learning_rate.set(0.1)
        self.discount_factor.set(0.99)
        self.epsilon_decay.set(0.9965)
        self.episodes.set(15000)
        self.update_frequency.set(100)
        self.map_size.set("4x4")
        self.is_slippery.set(True)
        self.on_map_size_change()
        self.log("\nğŸ¯ PRÃ‰RÃ‰GLAGE OPTIMAL chargÃ©")
        self.log("   â€¢ Avec glace glissante")
        self.log("   â€¢ 15000 Ã©pisodes (plus long)")
        self.log("   â€¢ RÃ©sultat attendu: 75-85% de rÃ©ussite")

    def start_training(self):
        """Start training in a separate thread."""
        if self.is_training:
            return

        self.is_training = True
        self.rewards_history = []
        self.last_graph_update = 0
        self.train_button.config(state=tk.DISABLED)
        self.stop_button.config(state=tk.NORMAL)
        self.demo_button.config(state=tk.DISABLED)

        self.training_thread = threading.Thread(target=self.train_agent, daemon=True)
        self.training_thread.start()

    def stop_training(self):
        """Stop training."""
        self.is_training = False
        self.log("â¹ï¸  ArrÃªt de l'entraÃ®nement demandÃ©...")

    def train_agent(self):
        """Train the agent (runs in separate thread - optimized for speed)."""
        try:
            # Get parameters
            map_name = self.map_size.get()
            is_slippery = self.is_slippery.get()
            lr = self.learning_rate.get()
            gamma = self.discount_factor.get()
            decay = self.epsilon_decay.get()
            episodes = self.episodes.get()
            update_freq = self.update_frequency.get()

            # Create environment
            if self.custom_map:
                self.env = gym.make("FrozenLake-v1", desc=self.custom_map, is_slippery=is_slippery)
            else:
                self.env = gym.make("FrozenLake-v1", map_name=map_name, is_slippery=is_slippery)

            # Get ludic parameters
            action_biases = [
                self.action_weights['left'].get(),
                self.action_weights['down'].get(),
                self.action_weights['right'].get(),
                self.action_weights['up'].get()
            ]
            reward_shaping = {
                'goal': self.rewards['goal'].get(),
                'hole': self.rewards['hole'].get(),
                'step': self.rewards['step'].get()
            }
            advanced_penalties = {
                'wall_hit': self.advanced_penalties['wall_hit'].get(),
                'revisit': self.advanced_penalties['revisit'].get(),
                'loop': self.advanced_penalties['loop'].get(),
                'distance': self.advanced_penalties['distance'].get()
            }

            # Create agent
            self.agent = QLearningAgent(
                self.env,
                learning_rate=lr,
                discount_factor=gamma,
                epsilon_decay=decay,
                action_biases=action_biases,
                reward_shaping=reward_shaping,
                advanced_penalties=advanced_penalties
            )

            self.message_queue.put(("log", f"\n{'='*72}"))
            self.message_queue.put(("log", f"ğŸš€ DÃ‰MARRAGE DE L'ENTRAÃNEMENT (Mode optimisÃ©)"))
            map_info = "PersonnalisÃ©e" if self.custom_map else map_name
            self.message_queue.put(("log", f"   Carte: {map_info} | Glissant: {'Oui' if is_slippery else 'Non'}"))
            self.message_queue.put(("log", f"   Î±={lr:.2f} | Î³={gamma:.2f} | decay={decay:.3f} | Ã©pisodes={episodes}"))

            # Show ludic parameters if non-default
            if action_biases != [1.0, 1.0, 1.0, 1.0]:
                self.message_queue.put(("log", f"   ğŸ® Poids actions: â†{action_biases[0]:.1f} â†“{action_biases[1]:.1f} â†’{action_biases[2]:.1f} â†‘{action_biases[3]:.1f}"))
            if reward_shaping != {'goal': 1.0, 'hole': 0.0, 'step': 0.0}:
                self.message_queue.put(("log", f"   ğŸ RÃ©compenses: Goal={reward_shaping['goal']:.1f} Hole={reward_shaping['hole']:.1f} Step={reward_shaping['step']:.2f}"))
            if advanced_penalties != {'wall_hit': 0.0, 'revisit': 0.0, 'loop': 0.0, 'distance': 0.0}:
                penalties_str = []
                if advanced_penalties['wall_hit'] != 0.0:
                    penalties_str.append(f"Mur={advanced_penalties['wall_hit']:.2f}")
                if advanced_penalties['revisit'] != 0.0:
                    penalties_str.append(f"Revisite={advanced_penalties['revisit']:.2f}")
                if advanced_penalties['loop'] != 0.0:
                    penalties_str.append(f"Boucle={advanced_penalties['loop']:.2f}")
                if advanced_penalties['distance'] != 0.0:
                    penalties_str.append(f"Distance={advanced_penalties['distance']:.2f}")
                if penalties_str:
                    self.message_queue.put(("log", f"   âš™ï¸ PÃ©nalitÃ©s: {' | '.join(penalties_str)}"))

            self.message_queue.put(("log", f"{'='*72}"))

            # Draw initial grid
            self.message_queue.put(("draw_grid", 0))

            start_time = time.time()
            last_update_time = start_time
            last_grid_update = start_time

            # Batch episodes for better performance
            batch_size = min(update_freq, 100)

            for episode in range(episodes):
                if not self.is_training:
                    break

                # Run episode
                state, info = self.env.reset()
                self.agent.reset_episode_tracking()  # Reset tracking for new episode
                done = False
                episode_reward = 0
                steps = 0
                max_steps = 100  # Prevent infinite loops

                while not done and steps < max_steps:
                    action = self.agent.choose_action(state)
                    next_state, reward, terminated, truncated, info = self.env.step(action)
                    done = terminated or truncated

                    self.agent.update_q_table(state, action, reward, next_state, done)

                    # Update grid visualization during training (throttled)
                    current_time = time.time()
                    if self.show_live_grid.get() and (current_time - last_grid_update) > 0.05:
                        self.message_queue.put(("draw_grid_training", state))
                        last_grid_update = current_time

                    state = next_state
                    episode_reward += reward
                    steps += 1

                self.rewards_history.append(episode_reward)
                self.agent.decay_epsilon()

                # Update UI less frequently and non-blocking
                current_time = time.time()
                if (episode + 1) % update_freq == 0 or (current_time - last_update_time) > 0.5:
                    last_update_time = current_time

                    # Calculate stats
                    recent_rewards = self.rewards_history[-100:] if len(self.rewards_history) >= 100 else self.rewards_history
                    avg_reward = np.mean(recent_rewards) if recent_rewards else 0
                    win_rate = (np.sum(recent_rewards) / len(recent_rewards) * 100) if recent_rewards else 0
                    elapsed = current_time - start_time

                    # Calculate ETA
                    episodes_per_sec = (episode + 1) / elapsed if elapsed > 0 else 0
                    remaining_episodes = episodes - (episode + 1)
                    eta_seconds = remaining_episodes / episodes_per_sec if episodes_per_sec > 0 else 0

                    # Send all updates at once (more efficient)
                    self.message_queue.put(("batch_update", {
                        "progress": (episode + 1, episodes),
                        "epsilon": self.agent.epsilon,
                        "winrate": win_rate,
                        "reward": avg_reward,
                        "time": elapsed,
                        "eta": eta_seconds
                    }))

                    if (episode + 1) % 1000 == 0:
                        self.message_queue.put(("log",
                            f"Ã‰pisode {episode + 1:5d}/{episodes} | "
                            f"RÃ©ussite: {win_rate:5.1f}% | "
                            f"Îµ={self.agent.epsilon:.3f} | "
                            f"Vitesse: {episodes_per_sec:.1f} ep/s"))

            # Training complete
            elapsed = time.time() - start_time
            final_rewards = self.rewards_history[-100:] if len(self.rewards_history) >= 100 else self.rewards_history
            final_winrate = (np.sum(final_rewards) / len(final_rewards) * 100) if final_rewards else 0
            avg_speed = episodes / elapsed if elapsed > 0 else 0

            # Calculate detailed statistics
            total_successes = int(np.sum(self.rewards_history))
            total_episodes = len(self.rewards_history)
            overall_winrate = (total_successes / total_episodes * 100) if total_episodes > 0 else 0

            # Calculate best performance window
            best_winrate = 0
            best_episode = 0
            window_size = min(100, total_episodes)
            for i in range(window_size, total_episodes + 1):
                window = self.rewards_history[i-window_size:i]
                winrate = (np.sum(window) / len(window) * 100)
                if winrate > best_winrate:
                    best_winrate = winrate
                    best_episode = i

            self.message_queue.put(("log", f"\n{'='*72}"))
            if self.is_training:
                self.message_queue.put(("log", f"âœ… ENTRAÃNEMENT TERMINÃ‰!"))
                self.message_queue.put(("log", f"{'='*72}"))
                self.message_queue.put(("log", f"ğŸ“Š STATISTIQUES FINALES:"))
                self.message_queue.put(("log", f""))
                self.message_queue.put(("log", f"   â±ï¸  Temps total: {elapsed:.1f}s ({avg_speed:.1f} Ã©pisodes/sec)"))
                self.message_queue.put(("log", f"   ğŸ¯ Ã‰pisodes: {total_episodes}"))
                self.message_queue.put(("log", f"   âœ… Victoires totales: {total_successes}/{total_episodes}"))
                self.message_queue.put(("log", f"   ğŸ“ˆ Taux global: {overall_winrate:.1f}%"))
                self.message_queue.put(("log", f"   ğŸ“Š Taux final (100 derniers): {final_winrate:.1f}%"))
                self.message_queue.put(("log", f"   ğŸ† Meilleur taux atteint: {best_winrate:.1f}% (Ã©pisode {best_episode})"))
                self.message_queue.put(("log", f"   ğŸ² Epsilon final: {self.agent.epsilon:.4f}"))
                self.message_queue.put(("log", f""))

                # Evaluation message
                if final_winrate > 80:
                    self.message_queue.put(("log", f"   ğŸŒŸ EXCELLENT! L'agent a trÃ¨s bien appris!"))
                elif final_winrate > 60:
                    self.message_queue.put(("log", f"   âœ… TRÃˆS BIEN! L'agent performe bien!"))
                elif final_winrate > 40:
                    self.message_queue.put(("log", f"   ğŸ‘ BIEN! Peut-Ãªtre essayer plus d'Ã©pisodes?"))
                else:
                    self.message_queue.put(("log", f"   âš ï¸  Apprentissage insuffisant. Essayez:"))
                    self.message_queue.put(("log", f"      - Augmenter les Ã©pisodes Ã  15000+"))
                    self.message_queue.put(("log", f"      - Augmenter le taux d'apprentissage"))
                    self.message_queue.put(("log", f"      - Ou utilisez le prÃ©rÃ©glage DÃ‰BUTANT"))

                self.message_queue.put(("log", f"\n   â¡ï¸  Cliquez sur 'ğŸ‘ï¸ DÃ©mo dans la Grille' pour voir l'agent!"))

                # Generate learning curve graph
                self.message_queue.put(("show_graph", None))
            else:
                self.message_queue.put(("log", f"â¹ï¸  EntraÃ®nement arrÃªtÃ© par l'utilisateur"))
                self.message_queue.put(("log", f"   Temps: {elapsed:.1f}s | RÃ©ussite: {final_winrate:.1f}%"))

            self.message_queue.put(("log", f"{'='*72}\n"))
            self.message_queue.put(("complete", None))

        except Exception as e:
            self.message_queue.put(("log", f"âŒ Erreur: {str(e)}"))
            import traceback
            self.message_queue.put(("log", traceback.format_exc()))
            self.message_queue.put(("complete", None))

    def show_grid_demo(self):
        """Show grid demo with the trained agent (animated in GUI)."""
        if self.agent is None:
            self.log("âŒ Aucun agent entraÃ®nÃ©! Lancez d'abord l'entraÃ®nement.")
            return

        self.log("\nğŸ¬ Lancement de la dÃ©mo dans la grille...")
        self.demo_button.config(state=tk.DISABLED)
        threading.Thread(target=self.run_grid_demo, daemon=True).start()

    def show_pygame_demo(self):
        """Show pygame demo with the trained agent (game window)."""
        if self.agent is None:
            self.log("âŒ Aucun agent entraÃ®nÃ©! Lancez d'abord l'entraÃ®nement.")
            return

        self.log("\nğŸ® Lancement de la dÃ©mo Pygame (fenÃªtre sÃ©parÃ©e)...")
        self.pygame_button.config(state=tk.DISABLED)

        # Use subprocess instead of threading to avoid macOS crash
        threading.Thread(target=self.run_pygame_demo_subprocess, daemon=True).start()

    def run_pygame_demo_subprocess(self):
        """Run pygame demo in a separate process (macOS compatible)."""
        import subprocess
        import pickle
        import tempfile
        import os

        try:
            # Create temporary files for Q-table and custom map
            with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as f:
                q_table_path = f.name
                pickle.dump(self.agent.q_table, f)

            custom_map_path = None
            if self.custom_map:
                with tempfile.NamedTemporaryFile(mode='wb', delete=False, suffix='.pkl') as f:
                    custom_map_path = f.name
                    pickle.dump(self.custom_map, f)

            # Get parameters
            map_name = self.map_size.get()
            is_slippery = str(self.is_slippery.get())
            delay = str(self.demo_speed.get())

            # Build command
            python_cmd = sys.executable
            script_path = os.path.join(os.path.dirname(__file__), 'pygame_demo.py')

            cmd = [python_cmd, script_path, q_table_path, map_name, is_slippery, delay]
            if custom_map_path:
                cmd.append(custom_map_path)

            # Run subprocess
            self.message_queue.put(("log", "ğŸ® Ouverture de la fenÃªtre pygame..."))
            result = subprocess.run(cmd, capture_output=True, text=True)

            # Log output
            if result.stdout:
                for line in result.stdout.strip().split('\n'):
                    self.message_queue.put(("log", line))

            if result.returncode != 0 and result.stderr:
                self.message_queue.put(("log", f"âŒ Erreur: {result.stderr}"))

            # Clean up temp files
            os.unlink(q_table_path)
            if custom_map_path:
                os.unlink(custom_map_path)

            self.message_queue.put(("log", "ğŸ DÃ©mo pygame terminÃ©e!"))
            self.message_queue.put(("pygame_complete", None))

        except Exception as e:
            self.message_queue.put(("log", f"âŒ Erreur pygame: {str(e)}"))
            self.message_queue.put(("pygame_complete", None))

    def show_result_screen(self, screen, success, episode, total_episodes, steps):
        """Display victory/defeat overlay on pygame window."""
        import pygame

        # Get screen dimensions
        width, height = screen.get_size()

        # Create semi-transparent overlay
        overlay = pygame.Surface((width, height))
        overlay.set_alpha(220)
        overlay.fill((0, 0, 0))
        screen.blit(overlay, (0, 0))

        # Fonts
        try:
            title_font = pygame.font.SysFont('arial', 72, bold=True)
            subtitle_font = pygame.font.SysFont('arial', 36, bold=True)
            info_font = pygame.font.SysFont('arial', 28)
            small_font = pygame.font.SysFont('arial', 24)
        except:
            title_font = pygame.font.Font(None, 72)
            subtitle_font = pygame.font.Font(None, 36)
            info_font = pygame.font.Font(None, 28)
            small_font = pygame.font.Font(None, 24)

        if success:
            # Victory screen - bright and celebratory
            bg_color = (34, 139, 34)  # Forest green
            border_color = (50, 205, 50)  # Lime green
            title_color = (255, 255, 100)  # Bright yellow

            title_text = "VICTOIRE!"
            emoji = "ğŸ‰"
            subtitle_text = f"But atteint en {steps} Ã©tape{'s' if steps > 1 else ''}!"
        else:
            # Defeat screen - clear but not too aggressive
            bg_color = (139, 0, 0)  # Dark red
            border_color = (220, 20, 60)  # Crimson
            title_color = (255, 200, 200)  # Light pink

            title_text = "DÃ‰FAITE"
            emoji = "ğŸ’€"
            subtitle_text = "TombÃ© dans un trou!"

        # Calculate box dimensions
        box_width = width - 120
        box_height = 320
        box_x = 60
        box_y = height // 2 - box_height // 2

        # Draw outer glow effect
        for i in range(5):
            glow_alpha = 30 - (i * 5)
            glow_surface = pygame.Surface((width, height), pygame.SRCALPHA)
            pygame.draw.rect(glow_surface, (*border_color, glow_alpha),
                           (box_x - i*2, box_y - i*2, box_width + i*4, box_height + i*4),
                           border_radius=25)
            screen.blit(glow_surface, (0, 0))

        # Draw main background box with gradient effect
        pygame.draw.rect(screen, bg_color, (box_x, box_y, box_width, box_height), border_radius=20)

        # Draw decorative border (thick)
        pygame.draw.rect(screen, border_color, (box_x, box_y, box_width, box_height), 5, border_radius=20)

        # Draw inner highlight
        pygame.draw.rect(screen, (255, 255, 255, 60),
                        (box_x + 10, box_y + 10, box_width - 20, box_height - 20),
                        2, border_radius=15)

        # Render emoji (larger)
        emoji_font = pygame.font.SysFont('arial', 100)
        emoji_render = emoji_font.render(emoji, True, (255, 255, 255))
        emoji_rect = emoji_render.get_rect(center=(width // 2, box_y + 80))
        screen.blit(emoji_render, emoji_rect)

        # Render title with shadow
        title_render = title_font.render(title_text, True, (0, 0, 0))  # Shadow
        title_rect = title_render.get_rect(center=(width // 2 + 2, box_y + 162))
        screen.blit(title_render, title_rect)

        title_render = title_font.render(title_text, True, title_color)  # Main text
        title_rect = title_render.get_rect(center=(width // 2, box_y + 160))
        screen.blit(title_render, title_rect)

        # Render subtitle
        subtitle_render = subtitle_font.render(subtitle_text, True, (255, 255, 255))
        subtitle_rect = subtitle_render.get_rect(center=(width // 2, box_y + 220))
        screen.blit(subtitle_render, subtitle_rect)

        # Draw separator line
        line_y = box_y + 260
        pygame.draw.line(screen, (255, 255, 255, 100),
                        (box_x + 40, line_y), (box_x + box_width - 40, line_y), 2)

        # Episode info at bottom
        episode_render = info_font.render(f"Ã‰pisode {episode}/{total_episodes}", True, (220, 220, 220))
        episode_rect = episode_render.get_rect(center=(width // 2, box_y + 285))
        screen.blit(episode_render, episode_rect)

        pygame.display.flip()

    def run_pygame_demo(self):
        """Run pygame demo in separate window."""
        import pygame

        try:
            map_name = self.map_size.get()
            is_slippery = self.is_slippery.get()
            delay = self.demo_speed.get()

            if self.custom_map:
                demo_env = gym.make("FrozenLake-v1", desc=self.custom_map,
                                  is_slippery=is_slippery, render_mode="human")
            else:
                demo_env = gym.make("FrozenLake-v1", map_name=map_name,
                                  is_slippery=is_slippery, render_mode="human")

            action_names = {0: "â†GAUCHE", 1: "â†“BAS", 2: "â†’DROITE", 3: "â†‘HAUT"}
            successes = 0
            quit_requested = False

            # Trigger initial render to create window
            demo_env.reset()
            demo_env.render()

            # Get pygame window - try different attributes
            pygame_window = None
            if hasattr(demo_env.unwrapped, 'window'):
                pygame_window = demo_env.unwrapped.window
            elif hasattr(demo_env.unwrapped, 'screen'):
                pygame_window = demo_env.unwrapped.screen
            else:
                # Get display surface directly
                pygame_window = pygame.display.get_surface()

            for episode in range(5):
                if quit_requested:
                    break

                state, info = demo_env.reset()
                self.message_queue.put(("log", f"\nğŸ® Pygame Ã‰pisode {episode + 1}/5"))
                done = False
                step = 0

                while not done and step < 100:
                    # Check for pygame events (window close, etc.)
                    for event in pygame.event.get():
                        if event.type == pygame.QUIT:
                            quit_requested = True
                            self.message_queue.put(("log", "ğŸ›‘ Fermeture demandÃ©e par l'utilisateur"))
                            break

                    if quit_requested:
                        break

                    time.sleep(delay)
                    action = np.argmax(self.agent.q_table[state])
                    self.message_queue.put(("log", f"   {action_names[action]}"))

                    next_state, reward, terminated, truncated, info = demo_env.step(action)
                    done = terminated or truncated

                    if done:
                        # Show result screen
                        if reward > 0:
                            self.message_queue.put(("log", f"   âœ… SUCCÃˆS en {step + 1} Ã©tapes!"))
                            if pygame_window:
                                self.show_result_screen(pygame_window, True, episode + 1, 5, step + 1)
                            successes += 1
                        else:
                            self.message_queue.put(("log", f"   âŒ DÃ‰FAITE aprÃ¨s {step + 1} Ã©tapes"))
                            if pygame_window:
                                self.show_result_screen(pygame_window, False, episode + 1, 5, step + 1)

                        # Wait to show result (2 seconds or user close)
                        start_wait = time.time()
                        while time.time() - start_wait < 2.0:
                            for event in pygame.event.get():
                                if event.type == pygame.QUIT:
                                    quit_requested = True
                                    break
                            if quit_requested:
                                break
                            time.sleep(0.1)

                    state = next_state
                    step += 1

                if quit_requested:
                    break

            demo_env.close()

            if quit_requested:
                self.message_queue.put(("log", f"\nğŸ›‘ Pygame fermÃ© par l'utilisateur. SuccÃ¨s: {successes}/5"))
            else:
                self.message_queue.put(("log", f"\nğŸ Pygame terminÃ©! SuccÃ¨s: {successes}/5"))
            self.message_queue.put(("pygame_complete", None))

        except Exception as e:
            self.message_queue.put(("log", f"âŒ Erreur pygame: {str(e)}"))
            import traceback
            self.message_queue.put(("log", traceback.format_exc()))
            self.message_queue.put(("pygame_complete", None))

    def run_grid_demo(self):
        """Run visual demo with animation in the grid (in separate thread)."""
        try:
            delay = self.demo_speed.get()
            action_names = {0: "â†GAUCHE", 1: "â†“BAS", 2: "â†’DROITE", 3: "â†‘HAUT"}
            successes = 0

            for episode in range(5):
                state, info = self.env.reset()
                self.message_queue.put(("log", f"\nğŸ® DÃ©mo Ã‰pisode {episode + 1}/5"))
                self.message_queue.put(("draw_grid", state))  # Show initial position

                done = False
                step = 0
                path = [state]

                while not done and step < 100:  # Safety limit
                    time.sleep(delay)
                    action = np.argmax(self.agent.q_table[state])
                    self.message_queue.put(("log", f"   Step {step + 1}: {action_names[action]}"))

                    next_state, reward, terminated, truncated, info = self.env.step(action)
                    done = terminated or truncated

                    path.append(next_state)
                    self.message_queue.put(("draw_grid", next_state))  # Animate movement

                    if done:
                        if reward > 0:
                            self.message_queue.put(("log", f"   âœ… SUCCÃˆS en {step + 1} Ã©tapes!"))
                            self.message_queue.put(("draw_grid_success", (next_state, path)))
                            successes += 1
                        else:
                            self.message_queue.put(("log", f"   âŒ Ã‰chec aprÃ¨s {step + 1} Ã©tapes"))
                            self.message_queue.put(("draw_grid_fail", next_state))
                        time.sleep(delay * 2)  # Pause to see result

                    state = next_state
                    step += 1

            self.message_queue.put(("log", f"\nğŸ DÃ©mo terminÃ©e! SuccÃ¨s: {successes}/5 ({successes*20}%)"))
            self.message_queue.put(("demo_complete", None))

        except Exception as e:
            self.message_queue.put(("log", f"âŒ Erreur dÃ©mo: {str(e)}"))
            self.message_queue.put(("demo_complete", None))

    def check_queue(self):
        """Check message queue and update UI (optimized)."""
        try:
            # Process multiple messages at once for better performance
            messages_processed = 0
            max_messages_per_cycle = 10  # Limit to avoid UI lag

            while not self.message_queue.empty() and messages_processed < max_messages_per_cycle:
                msg_type, data = self.message_queue.get_nowait()
                messages_processed += 1

                if msg_type == "log":
                    self.log(data)

                elif msg_type == "batch_update":
                    # Handle all updates at once (much faster!)
                    if "progress" in data:
                        episode, total = data["progress"]
                        self.progress_var.set(f"{episode} / {total} Ã©pisodes")
                        self.progress_bar['maximum'] = total
                        self.progress_bar['value'] = episode

                    if "epsilon" in data:
                        self.epsilon_var.set(f"{data['epsilon']:.3f}")

                    if "winrate" in data:
                        winrate = data["winrate"]
                        self.winrate_var.set(f"{winrate:.1f}%")
                        # Color code based on performance
                        if winrate > 70:
                            self.winrate_label.config(foreground='darkgreen')
                        elif winrate > 50:
                            self.winrate_label.config(foreground='green')
                        elif winrate > 30:
                            self.winrate_label.config(foreground='orange')
                        else:
                            self.winrate_label.config(foreground='red')

                    if "reward" in data:
                        self.reward_var.set(f"{data['reward']:.3f}")

                    if "time" in data:
                        elapsed = data["time"]
                        mins = int(elapsed // 60)
                        secs = int(elapsed % 60)
                        self.time_var.set(f"{mins}m {secs}s" if mins > 0 else f"{secs}s")

                    if "eta" in data:
                        eta = data["eta"]
                        if eta > 0:
                            mins = int(eta // 60)
                            secs = int(eta % 60)
                            self.eta_var.set(f"{mins}m {secs}s" if mins > 0 else f"{secs}s")
                        else:
                            self.eta_var.set("--")

                    # Draw curve only every 50 episodes to avoid lag and crashes on macOS
                    if len(self.rewards_history) > 0 and len(self.rewards_history) - self.last_graph_update >= 50:
                        try:
                            self.show_learning_curve()
                            self.last_graph_update = len(self.rewards_history)
                        except Exception as e:
                            # Silently ignore graph errors to prevent crashes
                            pass

                elif msg_type == "progress":
                    episode, total = data
                    self.progress_var.set(f"{episode} / {total} Ã©pisodes")
                    self.progress_bar['maximum'] = total
                    self.progress_bar['value'] = episode

                elif msg_type == "epsilon":
                    self.epsilon_var.set(f"{data:.3f}")

                elif msg_type == "winrate":
                    self.winrate_var.set(f"{data:.1f}%")
                    # Color code based on performance
                    if data > 70:
                        self.winrate_label.config(foreground='darkgreen')
                    elif data > 50:
                        self.winrate_label.config(foreground='green')
                    elif data > 30:
                        self.winrate_label.config(foreground='orange')
                    else:
                        self.winrate_label.config(foreground='red')

                elif msg_type == "reward":
                    self.reward_var.set(f"{data:.3f}")

                elif msg_type == "time":
                    mins = int(data // 60)
                    secs = int(data % 60)
                    self.time_var.set(f"{mins}m {secs}s" if mins > 0 else f"{secs}s")

                elif msg_type == "eta":
                    if data > 0:
                        mins = int(data // 60)
                        secs = int(data % 60)
                        self.eta_var.set(f"{mins}m {secs}s" if mins > 0 else f"{secs}s")
                    else:
                        self.eta_var.set("--")

                elif msg_type == "draw_curve":
                    pass  # Removed learning curve

                elif msg_type == "draw_grid_training":
                    # Draw grid during training (just agent position, no extras)
                    self.draw_grid(agent_pos=data)

                elif msg_type == "draw_grid":
                    # Draw grid with agent at position
                    self.draw_grid(agent_pos=data)

                elif msg_type == "draw_grid_success":
                    # Draw grid with path showing success
                    agent_pos, path = data
                    self.draw_grid(agent_pos=agent_pos, path=path)

                elif msg_type == "draw_grid_fail":
                    # Draw grid showing failure
                    self.draw_grid(agent_pos=data)

                elif msg_type == "demo_complete":
                    self.demo_button.config(state=tk.NORMAL)
                    # Show final grid with Q-values
                    if self.agent:
                        self.draw_grid(show_values=True)

                elif msg_type == "pygame_complete":
                    self.pygame_button.config(state=tk.NORMAL)

                elif msg_type == "show_graph":
                    self.show_learning_curve()

                elif msg_type == "complete":
                    self.is_training = False
                    self.train_button.config(state=tk.NORMAL)
                    self.stop_button.config(state=tk.DISABLED)
                    self.demo_button.config(state=tk.NORMAL)
                    self.pygame_button.config(state=tk.NORMAL)
                    # Show grid with learned policy
                    if self.env:
                        self.draw_grid(show_values=True)

        except Exception as e:
            print(f"Queue error: {e}")

        self.root.after(100, self.check_queue)


def main():
    """Launch the GUI application."""
    root = tk.Tk()
    app = FrozenLakeGUI(root)
    root.mainloop()


if __name__ == "__main__":
    main()
