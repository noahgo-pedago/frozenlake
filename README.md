# FrozenLake Gymnasium Project

A Python project implementing reinforcement learning agents for the Gymnasium FrozenLake environment.

## Overview

FrozenLake is a classic grid-world environment where an agent must navigate across a frozen lake from the starting position to the goal while avoiding holes in the ice. The lake is slippery, so the agent doesn't always move in the intended direction.

**Environment Details:**
- **Grid Size:** 4x4 (default) or 8x8
- **Actions:** 4 discrete actions (Left, Down, Right, Up)
- **States:** 16 positions (4x4 grid)
- **Objective:** Navigate from Start (S) to Goal (G) while avoiding Holes (H)

## Project Structure

```
frozenlake/
‚îú‚îÄ‚îÄ requirements.txt           # Project dependencies
‚îú‚îÄ‚îÄ run.sh                    # üöÄ Script de lancement rapide (Linux/Mac)
‚îú‚îÄ‚îÄ run.bat                   # üöÄ Script de lancement rapide (Windows CMD)
‚îú‚îÄ‚îÄ run.ps1                   # üöÄ Script de lancement rapide (Windows PowerShell)
‚îú‚îÄ‚îÄ frozenlake_gui.py         # üéì Interface graphique interactive (RECOMMAND√â!)
‚îú‚îÄ‚îÄ frozenlake_qlearning.py   # Q-learning agent implementation
‚îú‚îÄ‚îÄ frozenlake_random.py      # Random baseline agent
‚îú‚îÄ‚îÄ frozenlake_visual_demo.py # Visual demo with graphical rendering
‚îú‚îÄ‚îÄ venv/                     # Virtual environment
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üöÄ Lancement Rapide (Recommand√©!)

### Linux / Mac:
```bash
./run.sh
```

### Windows:

**Option 1 - Command Prompt (CMD):**
```cmd
run.bat
```
Double-cliquez sur `run.bat` ou ex√©cutez-le depuis CMD.

**Option 2 - PowerShell (recommand√©):**
```powershell
.\run.ps1
```
Clic-droit sur `run.ps1` ‚Üí "Ex√©cuter avec PowerShell"

> **Note PowerShell:** Si vous obtenez une erreur d'ex√©cution de script, ex√©cutez d'abord:
> ```powershell
> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
> ```

Ces scripts vont automatiquement:
- ‚úÖ Cr√©er le virtual environment (si n√©cessaire)
- ‚úÖ Installer les d√©pendances
- ‚úÖ Lancer l'interface graphique

**C'est la m√©thode la plus simple pour d√©marrer!**

## Installation Manuelle

Si vous pr√©f√©rez installer manuellement:

1. **Sur Linux, installer tkinter si n√©cessaire:**
```bash
# Ubuntu/Debian
sudo apt-get install python3-tk

# Fedora
sudo dnf install python3-tkinter

# Arch
sudo pacman -S tk
```

2. Create a virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

**Note:** Tkinter est inclus par d√©faut avec Python sur Windows et Mac. Sur Linux, il peut n√©cessiter une installation syst√®me.

## Usage

### üéì Interface Graphique Interactive (RECOMMAND√â!)

```bash
./run.sh          # Linux/Mac
run.bat           # Windows CMD
.\run.ps1         # Windows PowerShell
```

**Utilisation Simple en 3 √âtapes:**

1. **Choisir un pr√©r√©glage** (D√©butant, Standard ou Optimal)
2. **Cliquer sur "D√©marrer l'Entra√Ænement"**
3. **Regarder la d√©mo** une fois l'entra√Ænement termin√©

**Pr√©r√©glages Disponibles:**

| Pr√©r√©glage | √âpisodes | Difficult√© | R√©sultat attendu |
|------------|----------|------------|------------------|
| **D√©butant** | 5000 | Facile | >90% r√©ussite |
| **Standard** | 10000 | Moyenne | 65-75% r√©ussite |
| **Optimal** | 15000 | Difficile | 75-85% r√©ussite |

**Fonctionnalit√©s:**

- üó∫Ô∏è **Carte** - 4x4, 8x8 ou personnalis√©e
- ‚ùÑÔ∏è **Glace glissante** - Active/d√©sactive la stochasticit√©
- üìä **Statistiques en direct** - Progression, taux de r√©ussite, temps
- üìà **Graphique** - Courbe d'apprentissage en temps r√©el

**Contr√¥les de la D√©mo:**

| Contr√¥le | Description |
|----------|-------------|
| **Vitesse** | Curseur 0.05s - 1.0s (modifiable pendant la d√©mo) |
| **Max steps** | Limite de mouvements avant √©chec (10-200) |
| **Stop D√©mo** | Arr√™ter la d√©mo √† tout moment |

### Visual Demo (Command Line)

Watch the agent learn and play with graphical rendering:

```bash
python frozenlake_visual_demo.py
```

This interactive demo offers:
1. **Random Agent** - Watch an agent move randomly (no learning)
2. **Trained Agent** - See a Q-learning agent navigate successfully
3. **Compare Both** - See random vs trained side-by-side
4. **Quick Demo** - Fast 3-episode demonstration

The visual demo shows the grid with:
- **S** (Start) - Blue square where the agent begins
- **F** (Frozen) - White ice tiles (safe to walk on)
- **H** (Hole) - Black holes (game over if you fall in)
- **G** (Goal) - Green target to reach
- **Agent** - Red circle showing current position

### Train Q-Learning Agent

Train an agent using Q-learning algorithm:

```bash
python frozenlake_qlearning.py
```

This will:
- Train the agent for 10,000 episodes
- Print progress every 1,000 episodes
- Evaluate the trained agent over 100 test episodes
- Display final win rate and average reward

### Run Random Agent Baseline

See how a random agent performs:

```bash
python frozenlake_random.py
```

This demonstrates the baseline performance without learning.

## How It Works

### Q-Learning Algorithm

The Q-learning agent learns by:
1. **Exploration vs Exploitation:** Using epsilon-greedy policy to balance exploring new actions and exploiting learned knowledge
2. **Q-Table Updates:** Learning optimal action values using the Q-learning formula:
   ```
   Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
   ```
   where:
   - Œ± (alpha) = learning rate
   - Œ≥ (gamma) = discount factor
   - r = reward
   - s = current state
   - a = action taken
   - s' = next state

3. **Epsilon Decay:** Gradually reducing exploration as the agent learns

### Hyperparameters

Default parameters in `frozenlake_qlearning.py`:
- **Learning Rate (Œ±):** 0.1
- **Discount Factor (Œ≥):** 0.99
- **Initial Epsilon:** 1.0 (100% exploration)
- **Epsilon Decay:** 0.995
- **Minimum Epsilon:** 0.01
- **Training Episodes:** 10,000

**Comprendre les Hyperparam√®tres:**

| Param√®tre | Effet si trop bas | Effet si trop haut |
|-----------|-------------------|-------------------|
| **Learning Rate (Œ±)** | Apprentissage tr√®s lent | Apprentissage instable, oscillations |
| **Discount Factor (Œ≥)** | Agent myope, ignore le futur | Peut survaloriser des chemins longs |
| **Epsilon Decay** | Reste en exploration trop longtemps | Exploite trop t√¥t, manque de solutions |

## Customization

### Modify Hyperparameters

Edit the agent initialization in `frozenlake_qlearning.py`:

```python
agent = QLearningAgent(
    env=env,
    learning_rate=0.1,      # Adjust learning rate
    discount_factor=0.95,   # Adjust discount factor
    epsilon=1.0,            # Initial exploration rate
    epsilon_decay=0.99,     # Adjust decay rate
    epsilon_min=0.01        # Minimum exploration
)
```

### Use 8x8 Map

Change the environment creation to use a larger map:

```python
env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=True)
```

### Disable Slippery Ice

For deterministic movement:

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
```

## Expected Results

- **Random Agent:** ~1-2% win rate
- **Q-Learning Agent:** ~70-80% win rate (4x4 slippery map)

The Q-learning agent significantly outperforms random actions by learning optimal paths.

## Releases

### T√©l√©charger l'Application

Des ex√©cutables pr√©-compil√©s sont disponibles dans les [Releases](../../releases):

| Plateforme | Fichier |
|------------|---------|
| Windows | `FrozenLake-QLearning.exe` |
| macOS | `FrozenLake-macOS.zip` |
| Linux | `FrozenLake-QLearning` |

**Aucune installation requise** - tout est inclus dans l'ex√©cutable!

### Cr√©er une Release

Pour cr√©er une nouvelle release:

1. **Via tag Git:**
   ```bash
   git tag v1.0.0
   git push origin v1.0.0
   ```

2. **Via GitHub Actions:**
   - Aller dans Actions > "Build and Release"
   - Cliquer "Run workflow"
   - Entrer la version (ex: v1.0.0)

Le CI/CD g√©n√®re automatiquement les ex√©cutables pour Windows, macOS et Linux.

## Resources

- [Gymnasium FrozenLake Documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- [Q-Learning Algorithm](https://en.wikipedia.org/wiki/Q-learning)
- [Gymnasium Documentation](https://gymnasium.farama.org/)

## License

This project is open source and available for educational purposes.
