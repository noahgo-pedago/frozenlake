# FrozenLake Gymnasium Project

A Python project implementing reinforcement learning agents for the Gymnasium FrozenLake environment.

## Overview

FrozenLake is a classic grid-world environment where an agent must navigate across a frozen lake from the starting position to the goal while avoiding holes in the ice. The lake is slippery, so the agent doesn't always move in the intended direction.

**Environment Details:**
- **Grid Size:** 4x4 (default) or 8x8
- **Actions:** 4 discrete actions (Left, Down, Right, Up)
- **States:** 16 positions (4x4 grid)
- **Objective:** Navigate from Start (S) to Goal (G) while avoiding Holes (H)

## Project Structure

```
frozenlake/
‚îú‚îÄ‚îÄ requirements.txt           # Project dependencies
‚îú‚îÄ‚îÄ run.sh                    # üöÄ Script de lancement rapide (Linux/Mac)
‚îú‚îÄ‚îÄ run.bat                   # üöÄ Script de lancement rapide (Windows CMD)
‚îú‚îÄ‚îÄ run.ps1                   # üöÄ Script de lancement rapide (Windows PowerShell)
‚îú‚îÄ‚îÄ frozenlake_gui.py         # üéì Interface graphique interactive (RECOMMAND√â!)
‚îú‚îÄ‚îÄ frozenlake_qlearning.py   # Q-learning agent implementation
‚îú‚îÄ‚îÄ frozenlake_random.py      # Random baseline agent
‚îú‚îÄ‚îÄ frozenlake_visual_demo.py # Visual demo with graphical rendering
‚îú‚îÄ‚îÄ venv/                     # Virtual environment
‚îî‚îÄ‚îÄ README.md                 # This file
```

## üöÄ Lancement Rapide (Recommand√©!)

### Linux / Mac:
```bash
./run.sh
```

### Windows:

**Option 1 - Command Prompt (CMD):**
```cmd
run.bat
```
Double-cliquez sur `run.bat` ou ex√©cutez-le depuis CMD.

**Option 2 - PowerShell (recommand√©):**
```powershell
.\run.ps1
```
Clic-droit sur `run.ps1` ‚Üí "Ex√©cuter avec PowerShell"

> **Note PowerShell:** Si vous obtenez une erreur d'ex√©cution de script, ex√©cutez d'abord:
> ```powershell
> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
> ```

Ces scripts vont automatiquement:
- ‚úÖ Cr√©er le virtual environment (si n√©cessaire)
- ‚úÖ Installer les d√©pendances
- ‚úÖ Lancer l'interface graphique

**C'est la m√©thode la plus simple pour d√©marrer!**

## Installation Manuelle

Si vous pr√©f√©rez installer manuellement:

1. **Sur Linux, installer tkinter si n√©cessaire:**
```bash
# Ubuntu/Debian
sudo apt-get install python3-tk

# Fedora
sudo dnf install python3-tkinter

# Arch
sudo pacman -S tk
```

2. Create a virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

**Note:** Tkinter est inclus par d√©faut avec Python sur Windows et Mac. Sur Linux, il peut n√©cessiter une installation syst√®me.

## Usage

### üéì Interface Graphique Interactive (RECOMMAND√â pour les √âtudiants!)

Lancez l'interface graphique compl√®te pour exp√©rimenter avec les param√®tres d'apprentissage:

```bash
# Avec le script de lancement (plus simple)
./run.sh  # ou run.bat sur Windows

# Ou manuellement
source venv/bin/activate
python frozenlake_gui.py
```

**Fonctionnalit√©s de l'Interface:**

- ‚öôÔ∏è **Contr√¥les des Hyperparam√®tres** - Ajustez en temps r√©el:

  | Param√®tre | Plage | Description |
  |-----------|-------|-------------|
  | Taux d'apprentissage (Œ±) | 0.01 - 1.0 | Vitesse d'apprentissage de l'agent |
  | Facteur de discount (Œ≥) | 0.0 - 1.0 | Importance des r√©compenses futures |
  | D√©croissance epsilon | 0.9 - 0.999 | Vitesse de transition exploration ‚Üí exploitation |
  | Nombre d'√©pisodes | 1000 - 50000 | Dur√©e de l'entra√Ænement |
  | Taille de carte | 4x4 / 8x8 / Personnalis√©e | Complexit√© de l'environnement |
  | Glace glissante | On/Off | Stochasticit√© des mouvements |

- üìä **Statistiques en Temps R√©el:**
  - Progression de l'entra√Ænement (barre de progression)
  - Taux de r√©ussite (%)
  - Epsilon actuel (exploration vs exploitation)
  - R√©compense moyenne glissante
  - Temps √©coul√©
  - Graphique de progression en direct

- üéÆ **Pr√©r√©glages Pr√™ts √† l'Emploi** (ne modifient pas la carte):

  | Pr√©r√©glage | Œ± | Œ≥ | Œµ decay | √âpisodes |
  |------------|---|---|---------|----------|
  | **D√©butant** | 0.2 | 0.95 | 0.997 | 5000 |
  | **Standard** | 0.15 | 0.98 | 0.996 | 10000 |
  | **Optimal** | 0.1 | 0.99 | 0.9965 | 15000 |

- üó∫Ô∏è **√âditeur de Carte Personnalis√©e** - Cr√©ez vos propres environnements
- üëÅÔ∏è **D√©mo Visuelle Int√©gr√©e** - Regardez l'agent entra√Æn√© jouer

Cette interface est parfaite pour comprendre l'impact de chaque hyperparam√®tre sur l'apprentissage!

### Visual Demo (Command Line)

Watch the agent learn and play with graphical rendering:

```bash
python frozenlake_visual_demo.py
```

This interactive demo offers:
1. **Random Agent** - Watch an agent move randomly (no learning)
2. **Trained Agent** - See a Q-learning agent navigate successfully
3. **Compare Both** - See random vs trained side-by-side
4. **Quick Demo** - Fast 3-episode demonstration

The visual demo shows the grid with:
- **S** (Start) - Blue square where the agent begins
- **F** (Frozen) - White ice tiles (safe to walk on)
- **H** (Hole) - Black holes (game over if you fall in)
- **G** (Goal) - Green target to reach
- **Agent** - Red circle showing current position

### Train Q-Learning Agent

Train an agent using Q-learning algorithm:

```bash
python frozenlake_qlearning.py
```

This will:
- Train the agent for 10,000 episodes
- Print progress every 1,000 episodes
- Evaluate the trained agent over 100 test episodes
- Display final win rate and average reward

### Run Random Agent Baseline

See how a random agent performs:

```bash
python frozenlake_random.py
```

This demonstrates the baseline performance without learning.

## How It Works

### Q-Learning Algorithm

The Q-learning agent learns by:
1. **Exploration vs Exploitation:** Using epsilon-greedy policy to balance exploring new actions and exploiting learned knowledge
2. **Q-Table Updates:** Learning optimal action values using the Q-learning formula:
   ```
   Q(s,a) ‚Üê Q(s,a) + Œ±[r + Œ≥¬∑max(Q(s',a')) - Q(s,a)]
   ```
   where:
   - Œ± (alpha) = learning rate
   - Œ≥ (gamma) = discount factor
   - r = reward
   - s = current state
   - a = action taken
   - s' = next state

3. **Epsilon Decay:** Gradually reducing exploration as the agent learns

### Hyperparameters

Default parameters in `frozenlake_qlearning.py`:
- **Learning Rate (Œ±):** 0.1
- **Discount Factor (Œ≥):** 0.99
- **Initial Epsilon:** 1.0 (100% exploration)
- **Epsilon Decay:** 0.995
- **Minimum Epsilon:** 0.01
- **Training Episodes:** 10,000

**Comprendre les Hyperparam√®tres:**

| Param√®tre | Effet si trop bas | Effet si trop haut |
|-----------|-------------------|-------------------|
| **Learning Rate (Œ±)** | Apprentissage tr√®s lent | Apprentissage instable, oscillations |
| **Discount Factor (Œ≥)** | Agent myope, ignore le futur | Peut survaloriser des chemins longs |
| **Epsilon Decay** | Reste en exploration trop longtemps | Exploite trop t√¥t, manque de solutions |

## Customization

### Modify Hyperparameters

Edit the agent initialization in `frozenlake_qlearning.py`:

```python
agent = QLearningAgent(
    env=env,
    learning_rate=0.1,      # Adjust learning rate
    discount_factor=0.95,   # Adjust discount factor
    epsilon=1.0,            # Initial exploration rate
    epsilon_decay=0.99,     # Adjust decay rate
    epsilon_min=0.01        # Minimum exploration
)
```

### Use 8x8 Map

Change the environment creation to use a larger map:

```python
env = gym.make("FrozenLake-v1", map_name="8x8", is_slippery=True)
```

### Disable Slippery Ice

For deterministic movement:

```python
env = gym.make("FrozenLake-v1", is_slippery=False)
```

## Expected Results

- **Random Agent:** ~1-2% win rate
- **Q-Learning Agent:** ~70-80% win rate (4x4 slippery map)

The Q-learning agent significantly outperforms random actions by learning optimal paths.

## Resources

- [Gymnasium FrozenLake Documentation](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- [Q-Learning Algorithm](https://en.wikipedia.org/wiki/Q-learning)
- [Gymnasium Documentation](https://gymnasium.farama.org/)

## License

This project is open source and available for educational purposes.
